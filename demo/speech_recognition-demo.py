#!/usr/bin/env python3
"""
ç¦»çº¿è¯­éŸ³è¯†åˆ« - ä½¿ç”¨ Sherpa-ONNX
åŠŸèƒ½ï¼šè¯­éŸ³æ´»åŠ¨æ£€æµ‹ + æ‰¹é‡è¯†åˆ« + GPUåŠ é€Ÿ
"""

import sys
import time
import argparse
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    import sherpa_onnx
    import soundfile as sf
    import numpy as np
    import librosa
except ImportError as e:
    print(f"è¯·å®‰è£…ä¾èµ–: pip install -r requirements.txt")
    sys.exit(1)


def load_audio(file_path):
    """åŠ è½½éŸ³é¢‘æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    print(f"æ­£åœ¨åŠ è½½éŸ³é¢‘æ–‡ä»¶: {file_path}")
    
    if not Path(file_path).exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨librosaåŠ è½½ï¼Œæ”¯æŒæ›´å¤šæ ¼å¼
        samples, sample_rate = librosa.load(file_path, sr=None, mono=True)
        print(f"ä½¿ç”¨librosaåŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"librosaåŠ è½½å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨soundfile")
        try:
            samples, sample_rate = sf.read(file_path, dtype='float32')
            # è½¬æ¢ä¸ºå•å£°é“
            if len(samples.shape) > 1:
                samples = np.mean(samples, axis=1)
        except Exception as e2:
            raise RuntimeError(f"æ— æ³•åŠ è½½éŸ³é¢‘æ–‡ä»¶ {file_path}: {e2}")
    
    # VADéœ€è¦16kHzé‡‡æ ·ç‡ï¼Œå¦‚æœåŸå§‹é‡‡æ ·ç‡ä¸æ˜¯16kHzï¼Œéœ€è¦é‡é‡‡æ ·
    if sample_rate != 16000:
        print(f"åŸå§‹é‡‡æ ·ç‡: {sample_rate}Hz, é‡é‡‡æ ·åˆ°16kHz")
        samples = librosa.resample(samples, orig_sr=sample_rate, target_sr=16000)
        sample_rate = 16000
    
    duration = len(samples) / sample_rate
    print(f"éŸ³é¢‘ä¿¡æ¯: é‡‡æ ·ç‡={sample_rate}Hz, æ—¶é•¿={duration:.2f}ç§’")
    
    return samples, sample_rate


def create_recognizer(use_gpu=False, num_threads=4, language="auto"):
    """åˆ›å»ºè¯­éŸ³è¯†åˆ«å™¨"""
    print("æ­£åœ¨åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«æ¨¡å‹...")

    provider = "cuda" if use_gpu else "cpu"
    print(f"ä½¿ç”¨è®¡ç®—è®¾å¤‡: {provider}")
    print(f"è¯­è¨€è®¾ç½®: {language}")

    # éªŒè¯è¯­è¨€å‚æ•°
    valid_languages = ["auto", "zh", "en", "ja", "ko", "yue"]
    if language not in valid_languages:
        print(f"è­¦å‘Š: æ— æ•ˆè¯­è¨€ '{language}'ï¼Œä½¿ç”¨ 'auto' è‡ªåŠ¨æ£€æµ‹")
        language = "auto"

    # ä½¿ç”¨æœ¬åœ°SenseVoiceæ¨¡å‹ï¼ˆæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€ç²¤è¯­ï¼‰
    # æ–°ç‰ˆæœ¬2025-09-09åŒ…å«å®Œæ•´åŠŸèƒ½ï¼šASR + SER + AED + æ ‡ç‚¹
    # model_dir = "models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2025-09-09"
    model_dir = "models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17"
    recognizer = sherpa_onnx.OfflineRecognizer.from_sense_voice(
        model=f"{model_dir}/model.onnx",
        tokens=f"{model_dir}/tokens.txt",
        provider=provider,
        num_threads=num_threads,
        language=language,
        debug=False,
    )

    print("è¯­éŸ³è¯†åˆ«æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    return recognizer


def create_punctuation_processor(use_gpu=False, num_threads=2):
    """åˆ›å»ºæ ‡ç‚¹ç¬¦å·å¤„ç†å™¨"""
    print("æ­£åœ¨åˆå§‹åŒ–æ ‡ç‚¹æ¨¡å‹...")

    provider = "cuda" if use_gpu else "cpu"
    print(f"æ ‡ç‚¹æ¨¡å‹ä½¿ç”¨è®¡ç®—è®¾å¤‡: {provider}")

    # æ ‡ç‚¹æ¨¡å‹è·¯å¾„
    punct_model_dir = "sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12"

    try:
        # åˆ›å»ºæ ‡ç‚¹æ¨¡å‹é…ç½®
        model_config = sherpa_onnx.OfflinePunctuationModelConfig()
        model_config.ct_transformer = f"{punct_model_dir}/model.onnx"
        model_config.num_threads = num_threads
        model_config.provider = provider

        # åˆ›å»ºæ ‡ç‚¹é…ç½®
        punctuation_config = sherpa_onnx.OfflinePunctuationConfig()
        punctuation_config.model = model_config

        punctuation_processor = sherpa_onnx.OfflinePunctuation(punctuation_config)
        print("æ ‡ç‚¹æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        return punctuation_processor

    except Exception as e:
        print(f"æ ‡ç‚¹æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("å°†è·³è¿‡æ ‡ç‚¹å¤„ç†")
        return None


def create_speaker_recognition(use_gpu=False, num_threads=2, speaker_db_file=None):
    """åˆ›å»ºè¯´è¯äººè¯†åˆ«ç³»ç»Ÿ"""
    print("æ­£åœ¨åˆå§‹åŒ–è¯´è¯äººè¯†åˆ«ç³»ç»Ÿ...")

    provider = "cuda" if use_gpu else "cpu"
    print(f"è¯´è¯äººè¯†åˆ«ä½¿ç”¨è®¡ç®—è®¾å¤‡: {provider}")

    try:
        # åˆ›å»ºè¯´è¯äººåµŒå…¥æå–å™¨
        config = sherpa_onnx.SpeakerEmbeddingExtractorConfig()
        config.model = "models/3dspeaker_speech_campplus_sv_zh_en_16k-common_advanced.onnx"
        config.provider = provider
        config.num_threads = num_threads

        extractor = sherpa_onnx.SpeakerEmbeddingExtractor(config)

        # åˆ›å»ºè¯´è¯äººç®¡ç†å™¨
        manager = sherpa_onnx.SpeakerEmbeddingManager(extractor.dim)

        print(f"è¯´è¯äººè¯†åˆ«ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ (åµŒå…¥ç»´åº¦: {extractor.dim})")
        return extractor, manager

    except Exception as e:
        print(f"è¯´è¯äººè¯†åˆ«ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        print("å°†è·³è¿‡è¯´è¯äººè¯†åˆ«")
        return None, None


def register_speaker_from_audio(speaker_manager, speaker_extractor, speaker_name, audio_file):
    """ä»éŸ³é¢‘æ–‡ä»¶æ³¨å†Œè¯´è¯äºº"""
    try:
        # åŠ è½½éŸ³é¢‘
        samples, sample_rate = librosa.load(audio_file, sr=None, mono=True)

        # æå–åµŒå…¥
        stream = speaker_extractor.create_stream()
        stream.accept_waveform(sample_rate, samples)
        stream.input_finished()

        embedding = speaker_extractor.compute(stream)
        if isinstance(embedding, list):
            embedding = np.array(embedding)

        # æ³¨å†Œåˆ°ç®¡ç†å™¨
        success = speaker_manager.add(speaker_name, embedding.tolist())

        if success:
            print(f"âœ… è¯´è¯äºº '{speaker_name}' æ³¨å†ŒæˆåŠŸ")
            return True
        else:
            print(f"âŒ è¯´è¯äºº '{speaker_name}' æ³¨å†Œå¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ æ³¨å†Œè¯´è¯äºº '{speaker_name}' æ—¶å‡ºé”™: {e}")
        return False


def detect_speech_segments(samples, sample_rate):
    """ä½¿ç”¨Sherpa-ONNX VADè¿›è¡Œè¯­éŸ³æ®µè½æ£€æµ‹"""
    print("æ­£åœ¨è¿›è¡Œè¯­éŸ³æ®µè½æ£€æµ‹...")
    
    # åˆ›å»ºVADé…ç½®
    config = sherpa_onnx.VadModelConfig()
    config.silero_vad.model = "models/silero_vad.onnx"
    config.silero_vad.threshold = 0.5
    config.silero_vad.min_silence_duration = 0.25  # ç§’
    config.silero_vad.min_speech_duration = 0.25   # ç§’
    config.silero_vad.max_speech_duration = 5      # ç§’
    config.sample_rate = sample_rate
    
    window_size = config.silero_vad.window_size
    
    # åˆ›å»ºVADå®ä¾‹
    vad = sherpa_onnx.VoiceActivityDetector(config, buffer_size_in_seconds=30)
    
    segments = []
    total_samples_processed = 0
    
    # å¤„ç†éŸ³é¢‘æ•°æ®
    while len(samples) > total_samples_processed + window_size:
        chunk = samples[total_samples_processed:total_samples_processed + window_size]
        vad.accept_waveform(chunk)
        total_samples_processed += window_size
        
        # è·å–æ£€æµ‹åˆ°çš„è¯­éŸ³æ®µè½
        while not vad.empty():
            segment_samples = vad.front.samples
            start_time = vad.front.start / sample_rate
            duration = len(segment_samples) / sample_rate
            end_time = start_time + duration

            segments.append({
                'samples': segment_samples,
                'sample_rate': sample_rate,
                'start_time': start_time,
                'end_time': end_time
            })
            vad.pop()
    
    # å¤„ç†å‰©ä½™çš„éŸ³é¢‘æ•°æ®
    vad.flush()
    while not vad.empty():
        segment_samples = vad.front.samples
        start_time = vad.front.start / sample_rate
        duration = len(segment_samples) / sample_rate
        end_time = start_time + duration

        segments.append({
            'samples': segment_samples,
            'sample_rate': sample_rate,
            'start_time': start_time,
            'end_time': end_time
        })
        vad.pop()
    
    print(f"æ£€æµ‹åˆ° {len(segments)} ä¸ªè¯­éŸ³æ®µè½")
    return segments


def process_batch(recognizer, batch_segments, start_idx):
    """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„è¯­éŸ³æ®µè½"""
    # åˆ›å»ºè¯†åˆ«æµ - é¢„åˆ†é…å†…å­˜
    streams = [None] * len(batch_segments)

    for i, segment in enumerate(batch_segments):
        stream = recognizer.create_stream()
        # ç›´æ¥ä¼ é€’numpyæ•°ç»„ï¼Œé¿å…ä¸å¿…è¦çš„å¤åˆ¶
        stream.accept_waveform(segment['sample_rate'], segment['samples'])
        streams[i] = stream

    # æ‰¹é‡è¯†åˆ« - è¿™æ˜¯çœŸæ­£çš„å¹¶è¡Œå¤„ç†
    recognizer.decode_streams(streams)

    # æ‰¹é‡è·å–ç»“æœï¼ˆåŒ…å«å®Œæ•´åŠŸèƒ½ï¼šASR + SER + AEDï¼‰
    batch_results = []
    for i, stream in enumerate(streams):
        result = stream.result
        segment_idx = start_idx + i

        # å‡†å¤‡åŸºç¡€ç»“æœ
        result_data = {
            'text': result.text,
            'emotion': result.emotion,      # æƒ…æ„Ÿè¯†åˆ« (SER)
            'event': result.event,          # éŸ³é¢‘äº‹ä»¶æ£€æµ‹ (AED)
            'language': result.lang,        # è¯­è¨€è¯†åˆ«
            'start_time': batch_segments[i]['start_time'],
            'end_time': batch_segments[i]['end_time'],
            'segment_idx': segment_idx,     # ä¿å­˜åŸå§‹ç´¢å¼•ç”¨äºæ’åº
            'text_with_punct': result.text, # é»˜è®¤ä¸ºåŸå§‹æ–‡æœ¬ï¼Œå¦‚æœæ ‡ç‚¹å¤„ç†å¤±è´¥
            'speaker': 'Unknown'           # é»˜è®¤ä¸ºæœªçŸ¥è¯´è¯äºº
        }

        batch_results.append(result_data)

    return batch_results


def add_punctuation_to_results(results, punctuation_processor):
    """ä¸ºè¯†åˆ«ç»“æœæ·»åŠ æ ‡ç‚¹ç¬¦å·"""
    if not punctuation_processor:
        print("æ ‡ç‚¹å¤„ç†å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡æ ‡ç‚¹å¤„ç†")
        return results

    print("æ­£åœ¨ä¸ºè¯†åˆ«ç»“æœæ·»åŠ æ ‡ç‚¹ç¬¦å·...")

    try:
        for result in results:
            if result['text'].strip():  # åªå¤„ç†éç©ºæ–‡æœ¬
                # ä½¿ç”¨æ ‡ç‚¹å¤„ç†å™¨æ·»åŠ æ ‡ç‚¹
                punctuated_text = punctuation_processor.add_punctuation(result['text'])
                result['text_with_punct'] = punctuated_text
            else:
                result['text_with_punct'] = result['text']

        print(f"æ ‡ç‚¹å¤„ç†å®Œæˆï¼Œå·²å¤„ç† {len(results)} ä¸ªæ®µè½")
        return results

    except Exception as e:
        print(f"æ ‡ç‚¹å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("å°†ä½¿ç”¨åŸå§‹æ–‡æœ¬")
        return results


def add_speaker_recognition_to_results(results, speaker_extractor, speaker_manager, segments):
    """ä¸ºè¯†åˆ«ç»“æœæ·»åŠ è¯´è¯äººè¯†åˆ«"""
    if not speaker_extractor or not speaker_manager:
        print("è¯´è¯äººè¯†åˆ«ç³»ç»Ÿä¸å¯ç”¨ï¼Œè·³è¿‡è¯´è¯äººè¯†åˆ«")
        return results

    print("æ­£åœ¨è¿›è¡Œè¯´è¯äººè¯†åˆ«...")

    try:
        # ä¸´æ—¶è¯´è¯äººæ± å­ï¼šå­˜å‚¨æœ¬æ¬¡åˆ†æä¸­å‘ç°çš„æ–°è¯´è¯äººåµŒå…¥
        temp_speakers = []  # [(speaker_id, embedding), ...]
        temp_speaker_count = 0

        for i, result in enumerate(results):
            # è·å–å¯¹åº”çš„éŸ³é¢‘æ®µè½
            if i < len(segments):
                segment = segments[i]
                samples = segment['samples']
                sample_rate = segment['sample_rate']

                # æå–è¯´è¯äººåµŒå…¥
                stream = speaker_extractor.create_stream()
                stream.accept_waveform(sample_rate, samples)
                stream.input_finished()

                embedding = speaker_extractor.compute(stream)
                if isinstance(embedding, list):
                    embedding = np.array(embedding)

                embedding_list = embedding.tolist()

                # 1. é¦–å…ˆåœ¨å·²æ³¨å†Œçš„è¯´è¯äººæ± å­ä¸­æœç´¢
                registered_match = speaker_manager.search(embedding_list, threshold=0.3)  # ä½¿ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼

                if registered_match:
                    # æ‰¾åˆ°åŒ¹é…çš„å·²æ³¨å†Œè¯´è¯äºº
                    result['speaker'] = registered_match
                    continue

                # 2. åœ¨ä¸´æ—¶è¯´è¯äººæ± å­ä¸­æœç´¢ç›¸ä¼¼åº¦åŒ¹é…
                temp_match = None
                best_similarity = 0.0

                for temp_id, temp_embedding in temp_speakers:
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(embedding, temp_embedding) / (np.linalg.norm(embedding) * np.linalg.norm(temp_embedding))
                    if similarity > 0.6 and similarity > best_similarity:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                        temp_match = temp_id
                        best_similarity = similarity

                if temp_match:
                    # æ‰¾åˆ°åŒ¹é…çš„ä¸´æ—¶è¯´è¯äºº
                    result['speaker'] = temp_match
                else:
                    # æœªæ‰¾åˆ°åŒ¹é…ï¼Œåˆ›å»ºæ–°çš„ä¸´æ—¶è¯´è¯äºº
                    temp_speaker_count += 1
                    new_speaker_id = f"Speaker{temp_speaker_count}"
                    result['speaker'] = new_speaker_id

                    # å°†æ–°è¯´è¯äººåŠ å…¥ä¸´æ—¶æ± å­
                    temp_speakers.append((new_speaker_id, embedding))

        print(f"è¯´è¯äººè¯†åˆ«å®Œæˆï¼Œå·²å¤„ç† {len(results)} ä¸ªæ®µè½ï¼Œå‘ç° {temp_speaker_count} ä¸ªä¸åŒè¯´è¯äºº")
        return results

    except Exception as e:
        print(f"è¯´è¯äººè¯†åˆ«è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("å°†ä½¿ç”¨é»˜è®¤è¯´è¯äººæ ‡ç­¾")
        # å‡ºé”™æ—¶ä¸ºæ‰€æœ‰ç»“æœè®¾ç½®é»˜è®¤æ ‡ç­¾
        for i, result in enumerate(results):
            result['speaker'] = f"Speaker{i+1}"
        return results


def recognize_segments(recognizer, segments, batch_size=None, max_workers=None, punctuation_processor=None, speaker_extractor=None, speaker_manager=None):
    """æ‰¹é‡è¯†åˆ«è¯­éŸ³æ®µè½ - å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬"""
    print("æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«...")

    if not segments:
        return []

    total_segments = len(segments)

    # æ™ºèƒ½é€‰æ‹©æ‰¹æ¬¡å¤§å°å’Œworkeræ•°é‡
    if batch_size is None or max_workers is None:
        # æ ¹æ®æ®µè½æ€»æ•°æ™ºèƒ½é€‰æ‹©å‚æ•°
        if total_segments <= 10:
            # å°é‡æ•°æ®ï¼šç›´æ¥å¤„ç†ï¼Œä¸åˆ†æ‰¹
            batch_size = total_segments
            max_workers = 1
        elif total_segments <= 50:
            # ä¸­ç­‰æ•°æ®ï¼šå°æ‰¹æ¬¡ï¼Œé€‚é‡å¹¶è¡Œ
            batch_size = max(5, total_segments // 4)
            max_workers = min(4, total_segments // batch_size + 1)
        elif total_segments <= 200:
            # å¤§é‡æ•°æ®ï¼šä¸­ç­‰æ‰¹æ¬¡ï¼Œä¸­ç­‰å¹¶è¡Œ
            batch_size = max(10, total_segments // 8)
            max_workers = min(8, total_segments // batch_size + 1)
        else:
            # æµ·é‡æ•°æ®ï¼šå¤§æ‰¹é‡ï¼Œé«˜å¹¶è¡Œ
            batch_size = max(20, total_segments // 16)
            max_workers = min(16, total_segments // batch_size + 1)

    print(f"ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}ï¼Œæ€»æ®µè½æ•°: {total_segments}")

    # åˆ›å»ºæ‰¹æ¬¡ä»»åŠ¡
    batch_tasks = []
    for start_idx in range(0, total_segments, batch_size):
        end_idx = min(start_idx + batch_size, total_segments)
        current_batch = segments[start_idx:end_idx]
        batch_tasks.append((current_batch, start_idx))

    # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
    all_results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_batch = {
            executor.submit(process_batch, recognizer, batch, start_idx): (batch_idx, start_idx)
            for batch_idx, (batch, start_idx) in enumerate(batch_tasks)
        }

        # æ”¶é›†ç»“æœ
        completed = 0
        for future in as_completed(future_to_batch):
            batch_idx, start_idx = future_to_batch[future]
            try:
                batch_results = future.result()
                all_results.extend(batch_results)
                completed += 1
                print(f"å®Œæˆæ‰¹æ¬¡ {completed}/{len(batch_tasks)} (æ®µè½ {start_idx+1}-{start_idx+len(batch_results)})")
            except Exception as exc:
                print(f'æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {exc}')

    # æŒ‰ç…§æ—¶é—´é¡ºåºé‡æ–°æ’åºç»“æœ
    all_results.sort(key=lambda x: x['start_time'])

    # ç§»é™¤è¾…åŠ©å­—æ®µ
    for result in all_results:
        del result['segment_idx']

    # å¦‚æœæä¾›äº†æ ‡ç‚¹å¤„ç†å™¨ï¼Œä¸ºç»“æœæ·»åŠ æ ‡ç‚¹
    if punctuation_processor:
        all_results = add_punctuation_to_results(all_results, punctuation_processor)

    # å¦‚æœæä¾›äº†è¯´è¯äººè¯†åˆ«ç³»ç»Ÿï¼Œä¸ºç»“æœæ·»åŠ è¯´è¯äººä¿¡æ¯
    if speaker_extractor and speaker_manager:
        all_results = add_speaker_recognition_to_results(all_results, speaker_extractor, speaker_manager, segments)

    print(f"è¯­éŸ³è¯†åˆ«å®Œæˆï¼Œå¤„ç†äº† {len(all_results)} ä¸ªæ®µè½")
    return all_results


def print_results(results):
    """æ‰“å°è¯†åˆ«ç»“æœï¼ˆåŒ…å«æƒ…æ„Ÿåˆ†æï¼‰"""
    print("\n" + "="*80)
    print("ğŸ­ SenseVoiceå®Œæ•´åŠŸèƒ½è¯†åˆ«ç»“æœ (2025-09-09)")
    print("="*80)

    # ç»Ÿè®¡ä¿¡æ¯
    emotion_stats = {}
    language_stats = {}
    event_stats = {}

    try:
        for i, result in enumerate(results, 1):
            # ç»Ÿè®¡ä¿¡æ¯
            emotion = result.get('emotion', '<|UNKNOWN|>').strip('<|>').lower()
            language = result.get('language', '<|UNKNOWN|>').strip('<|>')
            event = result.get('event', '<|UNKNOWN|>').strip('<|>')

            emotion_stats[emotion] = emotion_stats.get(emotion, 0) + 1
            language_stats[language] = language_stats.get(language, 0) + 1
            event_stats[event] = event_stats.get(event, 0) + 1

            # æ˜¾ç¤ºç»“æœ
            print(f"[{i:02d}] {result['start_time']:.2f}s - {result['end_time']:.2f}s")
            print(f"     ğŸ“ åŸæ–‡: {result['text']}")
            if 'text_with_punct' in result and result['text_with_punct'] != result['text']:
                print(f"     âœï¸  æ ‡ç‚¹: {result['text_with_punct']}")
            print(f"     ğŸ˜Š æƒ…æ„Ÿ: {result.get('emotion', 'N/A')}")
            print(f"     ğŸ¯ äº‹ä»¶: {result.get('event', 'N/A')}")
            print(f"     ğŸŒ è¯­è¨€: {result.get('language', 'N/A')}")
            print(f"     ğŸ‘¤ è¯´è¯äºº: {result.get('speaker', 'Unknown')}")
            print()

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("-" * 80)
        print("ğŸ“Š åˆ†æç»Ÿè®¡:")

        if emotion_stats:
            print("   æƒ…æ„Ÿåˆ†å¸ƒ:")
            for emotion, count in sorted(emotion_stats.items()):
                percentage = (count / len(results)) * 100
                emoji_map = {
                    'happy': 'ğŸ˜Š',
                    'sad': 'ğŸ˜¢',
                    'angry': 'ğŸ˜ ',
                    'neutral': 'ğŸ˜',
                    'fearful': 'ğŸ˜¨',
                    'disgusted': 'ğŸ¤¢',
                    'surprised': 'ğŸ˜²',
                    'unknown': 'ğŸ¤”'
                }
                emoji = emoji_map.get(emotion, 'ğŸ¤”')
                print(f"     {emoji} {emotion.capitalize()}: {count} ({percentage:.1f}%)")

        if language_stats:
            print("   è¯­è¨€åˆ†å¸ƒ:")
            for lang, count in sorted(language_stats.items()):
                percentage = (count / len(results)) * 100
                print(f"     ğŸŒ {lang}: {count} ({percentage:.1f}%)")

        if event_stats:
            print("   äº‹ä»¶åˆ†å¸ƒ:")
            for event, count in sorted(event_stats.items()):
                percentage = (count / len(results)) * 100
                print(f"     ğŸ¯ {event}: {count} ({percentage:.1f}%)")

    except BrokenPipeError:
        # å¤„ç†ç®¡é“è¢«å…³é—­çš„æƒ…å†µï¼ˆå¦‚ä½¿ç”¨headå‘½ä»¤ï¼‰
        # è¿™æ˜¯æ­£å¸¸è¡Œä¸ºï¼Œä¸éœ€è¦æŠ¥é”™
        pass


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç¦»çº¿è¯­éŸ³è¯†åˆ«")
    parser.add_argument("input", help="è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--use-gpu", action="store_true", help="ä½¿ç”¨GPUåŠ é€Ÿ")
    parser.add_argument("--num-threads", type=int, default=4, help="CPUçº¿ç¨‹æ•°")
    parser.add_argument("--batch-size", type=int, default=None, help="æ‰¹é‡å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æ®µè½ä¸€èµ·å¤„ç†")
    parser.add_argument("--max-workers", type=int, default=None, help="æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°*2")
    parser.add_argument("--language", type=str, default="zh",
                       choices=["auto", "zh", "en", "ja", "ko", "yue"],
                       help="æŒ‡å®šè¯†åˆ«è¯­è¨€ (auto=è‡ªåŠ¨æ£€æµ‹, zh=ä¸­æ–‡, en=è‹±æ–‡, ja=æ—¥æ–‡, ko=éŸ©æ–‡, yue=ç²¤è¯­)")
    parser.add_argument("--enable-punctuation", action="store_true",
                       help="å¯ç”¨æ ‡ç‚¹ç¬¦å·å¤„ç†åŠŸèƒ½")
    parser.add_argument("--enable-speaker-id", action="store_true",
                       help="å¯ç”¨è¯´è¯äººè¯†åˆ«åŠŸèƒ½")
    parser.add_argument("--register-speaker", nargs=2, metavar=('NAME', 'AUDIO_FILE'),
                       help="æ³¨å†Œè¯´è¯äººï¼š--register-speaker å¼ ä¸‰ audio.wav")

    args = parser.parse_args()

    print("="*60)
    print("ğŸ­ SenseVoice å®Œæ•´åŠŸèƒ½è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ")
    print("   æ”¯æŒï¼šASR + SER + AED + æ ‡ç‚¹ç¬¦å· + è¯´è¯äººè¯†åˆ«")
    print("="*60)

    try:
        # 1. åŠ è½½éŸ³é¢‘
        samples, sample_rate = load_audio(args.input)

        # 2. åˆ›å»ºè¯†åˆ«å™¨
        recognizer = create_recognizer(args.use_gpu, args.num_threads, args.language)

        # 3. åˆ›å»ºæ ‡ç‚¹å¤„ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        punctuation_processor = None
        if args.enable_punctuation:
            punctuation_processor = create_punctuation_processor(args.use_gpu)

        # 4. åˆ›å»ºè¯´è¯äººè¯†åˆ«ç³»ç»Ÿï¼ˆå¦‚æœå¯ç”¨æˆ–éœ€è¦æ³¨å†Œï¼‰
        speaker_extractor, speaker_manager = None, None
        if args.enable_speaker_id or args.register_speaker:
            speaker_extractor, speaker_manager = create_speaker_recognition(args.use_gpu)

            # å¦‚æœæœ‰æ³¨å†Œè¯´è¯äººçš„è¯·æ±‚
            if args.register_speaker:
                speaker_name, speaker_audio = args.register_speaker
                success = register_speaker_from_audio(speaker_manager, speaker_extractor, speaker_name, speaker_audio)
                if success:
                    print(f"ğŸ¯ è¯´è¯äºº '{speaker_name}' å·²æ³¨å†Œ")
                else:
                    print(f"âŒ è¯´è¯äºº '{speaker_name}' æ³¨å†Œå¤±è´¥")

                # å¦‚æœåªæ˜¯æ³¨å†Œï¼Œä¸ç»§ç»­è¿›è¡Œè¯†åˆ«
                if not args.enable_speaker_id:
                    return

        # 5. æ£€æµ‹è¯­éŸ³æ®µè½
        segments = detect_speech_segments(samples, sample_rate)

        if not segments:
            print("æœªæ£€æµ‹åˆ°è¯­éŸ³å†…å®¹")
            return

        # 6. è¯†åˆ«è¯­éŸ³æ®µè½
        start_time = time.time()
        results = recognize_segments(recognizer, segments, args.batch_size, args.max_workers, punctuation_processor, speaker_extractor, speaker_manager)
        end_time = time.time()

        # 7. æ‰“å°ç»“æœ
        print_results(results)

        # 8. ç»Ÿè®¡ä¿¡æ¯
        try:
            total_duration = len(samples) / sample_rate
            processing_time = end_time - start_time
            rtf = processing_time / total_duration

            print(f"ğŸ¯ å¤„ç†ç»Ÿè®¡:")
            print(f"   éŸ³é¢‘æ—¶é•¿: {total_duration:.2f}ç§’")
            print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            print(f"   å®æ—¶å› å­: {rtf:.2f}")
            print(f"   è¯†åˆ«æ®µè½: {len(results)}ä¸ª")
            if punctuation_processor:
                print(f"   æ ‡ç‚¹å¤„ç†: âœ… å·²å¯ç”¨")
            else:
                print(f"   æ ‡ç‚¹å¤„ç†: âŒ æœªå¯ç”¨")

            if speaker_extractor and speaker_manager:
                print(f"   è¯´è¯äººè¯†åˆ«: âœ… å·²å¯ç”¨")
            else:
                print(f"   è¯´è¯äººè¯†åˆ«: âŒ æœªå¯ç”¨")
        except BrokenPipeError:
            # å¤„ç†ç®¡é“è¢«å…³é—­çš„æƒ…å†µ
            pass

    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
