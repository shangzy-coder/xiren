#!/usr/bin/env python3
"""
åŸºäºSenseVoiceæ¨¡å‹çš„ä¼ªæµå¼è¯­éŸ³è¯†åˆ«æ¼”ç¤º

ç”±äºSenseVoiceæ˜¯ç¦»çº¿æ¨¡å‹ï¼Œæ­¤æ¼”ç¤ºé€šè¿‡VADåˆ†æ®µå®ç°ä¼ªæµå¼æ•ˆæœï¼Œ
æä¾›æ¥è¿‘å®æ—¶çš„è¯­éŸ³è¯†åˆ«ä½“éªŒã€‚
"""

import argparse
import queue
import threading
import time
import numpy as np
import pyaudio
import sherpa_onnx


class StreamingSenseVoice:
    def __init__(self, model_dir, use_gpu=False, sample_rate=16000):
        """
        åˆå§‹åŒ–ä¼ªæµå¼SenseVoiceç³»ç»Ÿ
        
        Args:
            model_dir: SenseVoiceæ¨¡å‹ç›®å½•è·¯å¾„
            use_gpu: æ˜¯å¦ä½¿ç”¨GPU
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡
        """
        self.sample_rate = sample_rate
        self.use_gpu = use_gpu
        
        # åˆ›å»ºç¦»çº¿è¯†åˆ«å™¨é…ç½®
        recognizer_config = sherpa_onnx.OfflineRecognizerConfig(
            feat_config=sherpa_onnx.FeatureExtractorConfig(
                sampling_rate=sample_rate,
                feature_dim=80
            ),
            model_config=sherpa_onnx.OfflineModelConfig(
                sense_voice=sherpa_onnx.OfflineSenseVoiceModelConfig(
                    model=f"{model_dir}/model.int8.onnx",
                    language="auto",
                    use_itn=True
                ),
                tokens=f"{model_dir}/tokens.txt",
                num_threads=4,
                provider="cuda" if use_gpu else "cpu",
                model_type="sense_voice"
            )
        )
        
        # åˆ›å»ºè¯†åˆ«å™¨
        self.recognizer = sherpa_onnx.OfflineRecognizer(recognizer_config)
        
        # åˆ›å»ºVADé…ç½®
        vad_config = sherpa_onnx.VadModelConfig(
            silero_vad=sherpa_onnx.SileroVadModelConfig(
                model="models/silero_vad.onnx",
                threshold=0.5,
                min_silence_duration=0.25,
                min_speech_duration=0.25,
                window_size=512
            ),
            sample_rate=sample_rate,
            num_threads=2,
            provider="cpu"
        )
        
        # åˆ›å»ºVAD
        self.vad = sherpa_onnx.VoiceActivityDetector(vad_config, buffer_size_in_seconds=30)
        
        # éŸ³é¢‘ç¼“å†²åŒºå’Œé˜Ÿåˆ—
        self.audio_queue = queue.Queue()
        self.audio_buffer = []
        self.is_recording = False
        self.buffer_lock = threading.Lock()
        
        print(f"SenseVoiceæ¨¡å‹å·²åŠ è½½: {model_dir}")
        print(f"ä½¿ç”¨è®¾å¤‡: {'CUDA' if use_gpu else 'CPU'}")
        print(f"é‡‡æ ·ç‡: {sample_rate}Hz")
        print("=" * 50)
    
    def audio_callback(self, in_data, frame_count, time_info, status):
        """PyAudioå›è°ƒå‡½æ•°"""
        if self.is_recording:
            # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºfloat32å¹¶æ”¾å…¥é˜Ÿåˆ—
            audio_data = np.frombuffer(in_data, dtype=np.int16).astype(np.float32) / 32768.0
            self.audio_queue.put(audio_data)
        return (None, pyaudio.paContinue)
    
    def find_microphone_device(self):
        """è‡ªåŠ¨æ‰¾åˆ°æœ€ä½³çš„éº¦å…‹é£è®¾å¤‡"""
        audio = pyaudio.PyAudio()
        
        print("æ‰«æå¯ç”¨çš„éŸ³é¢‘è®¾å¤‡...")
        device_count = audio.get_device_count()
        best_device = None
        best_score = -1
        
        for i in range(device_count):
            info = audio.get_device_info_by_index(i)
            if info['maxInputChannels'] > 0:  # è¾“å…¥è®¾å¤‡
                score = 0
                name = info['name'].lower()
                
                # ä¼˜å…ˆé€‰æ‹©æ”¯æŒ16kHzçš„è®¾å¤‡
                if info['defaultSampleRate'] == 16000:
                    score += 100
                elif info['defaultSampleRate'] == 48000:
                    score += 50
                
                # é¿å…è™šæ‹Ÿè®¾å¤‡
                if 'pulse' not in name and 'virtual' not in name:
                    score += 20
                
                # ä¼˜å…ˆé€‰æ‹©USBæˆ–å†…ç½®éº¦å…‹é£
                if any(keyword in name for keyword in ['usb', 'microphone', 'mic']):
                    score += 10
                
                print(f"è®¾å¤‡ {i}: {info['name']} (é‡‡æ ·ç‡: {info['defaultSampleRate']}, å¾—åˆ†: {score})")
                
                if score > best_score:
                    best_score = score
                    best_device = i
        
        audio.terminate()
        
        if best_device is not None:
            print(f"é€‰æ‹©è®¾å¤‡: {best_device}")
            return best_device
        else:
            print("æœªæ‰¾åˆ°åˆé€‚çš„éº¦å…‹é£è®¾å¤‡ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡")
            return None
    
    def start_recording(self, device_id=None):
        """å¼€å§‹å½•éŸ³"""
        if device_id is None:
            device_id = self.find_microphone_device()
        
        # åˆå§‹åŒ–PyAudio
        self.audio = pyaudio.PyAudio()
        
        # è·å–è®¾å¤‡ä¿¡æ¯
        if device_id is not None:
            device_info = self.audio.get_device_info_by_index(device_id)
            input_sample_rate = int(device_info['defaultSampleRate'])
            print(f"ä½¿ç”¨è®¾å¤‡: {device_info['name']} (é‡‡æ ·ç‡: {input_sample_rate}Hz)")
        else:
            input_sample_rate = 16000
            print("ä½¿ç”¨é»˜è®¤è®¾å¤‡")
        
        # æ‰“å¼€éŸ³é¢‘æµ
        self.audio_stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=input_sample_rate,
            input=True,
            input_device_index=device_id,
            frames_per_buffer=1024,
            stream_callback=self.audio_callback
        )
        
        self.input_sample_rate = input_sample_rate
        self.is_recording = True
        self.audio_stream.start_stream()
        
        print("å¼€å§‹å½•éŸ³...")
        print("è¯´è¯æ—¶ä¼šè¿›è¡Œå®æ—¶VADæ£€æµ‹å’Œè¯­éŸ³è¯†åˆ«ï¼ŒæŒ‰Ctrl+Cç»“æŸ")
        print("-" * 50)
    
    def resample_audio(self, audio_data, input_rate, output_rate):
        """éŸ³é¢‘é‡é‡‡æ ·"""
        if input_rate == output_rate:
            return audio_data
        
        # ç®€å•çš„çº¿æ€§æ’å€¼é‡é‡‡æ ·
        input_length = len(audio_data)
        output_length = int(input_length * output_rate / input_rate)
        
        indices = np.linspace(0, input_length - 1, output_length)
        resampled = np.interp(indices, np.arange(input_length), audio_data)
        
        return resampled.astype(np.float32)
    
    def recognize_segment(self, audio_segment):
        """è¯†åˆ«éŸ³é¢‘æ®µè½"""
        try:
            # åˆ›å»ºç¦»çº¿æµ
            stream = self.recognizer.create_stream()
            stream.accept_waveform(self.sample_rate, audio_segment)
            
            # æ‰§è¡Œè¯†åˆ«
            self.recognizer.decode_stream(stream)
            result = stream.result
            
            return {
                'text': result.text,
                'language': getattr(result, 'lang', 'unknown'),
                'emotion': getattr(result, 'emotion', 'unknown')
            }
        except Exception as e:
            print(f"è¯†åˆ«å‡ºé”™: {e}")
            return {'text': '', 'language': 'unknown', 'emotion': 'unknown'}
    
    def process_audio(self):
        """å¤„ç†éŸ³é¢‘æ•°æ®çš„ä¸»å¾ªç¯"""
        print("ğŸ¤ æ­£åœ¨ç›‘å¬...")
        
        while self.is_recording:
            try:
                # ä»é˜Ÿåˆ—è·å–éŸ³é¢‘æ•°æ®
                audio_chunk = self.audio_queue.get(timeout=0.1)
                
                # é‡é‡‡æ ·åˆ°16kHzï¼ˆå¦‚æœéœ€è¦ï¼‰
                if self.input_sample_rate != self.sample_rate:
                    audio_chunk = self.resample_audio(
                        audio_chunk, self.input_sample_rate, self.sample_rate
                    )
                
                # æ·»åŠ åˆ°ç¼“å†²åŒº
                with self.buffer_lock:
                    self.audio_buffer.extend(audio_chunk)
                
                # å‘é€åˆ°VAD
                self.vad.accept_waveform(audio_chunk)
                
                # æ£€æŸ¥VADç»“æœ
                if self.vad.is_speech_detected():
                    print("ğŸ”Š æ£€æµ‹åˆ°è¯­éŸ³...", end="\r", flush=True)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„è¯­éŸ³æ®µè½
                if not self.vad.empty():
                    speech_segment = self.vad.front()
                    
                    # ä½¿ç”¨ç¼“å†²åŒºçš„éŸ³é¢‘è¿›è¡Œè¯†åˆ«
                    with self.buffer_lock:
                        if len(self.audio_buffer) > 8000:  # è‡³å°‘0.5ç§’çš„éŸ³é¢‘
                            audio_array = np.array(self.audio_buffer, dtype=np.float32)
                            
                            # æ¸…ç†è¾“å‡ºå¹¶æ˜¾ç¤ºå¤„ç†ä¸­
                            print(" " * 50, end="\r")
                            print("ğŸ”„ æ­£åœ¨è¯†åˆ«...", end="", flush=True)
                            
                            # è¯†åˆ«éŸ³é¢‘æ®µè½
                            result = self.recognize_segment(audio_array)
                            
                            # æ˜¾ç¤ºç»“æœ
                            if result['text'].strip():
                                print(f"\râœ“ è¯†åˆ«ç»“æœ: {result['text']}")
                                if result['language'] != 'unknown':
                                    print(f"  è¯­è¨€: {result['language']}")
                                if result['emotion'] != 'unknown':
                                    print(f"  æƒ…æ„Ÿ: {result['emotion']}")
                                print("-" * 30)
                            else:
                                print("\râŒ æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³")
                            
                            # æ¸…ç©ºç¼“å†²åŒºï¼Œä¿ç•™æœ€è¿‘çš„ä¸€å°éƒ¨åˆ†
                            keep_samples = int(self.sample_rate * 0.5)  # ä¿ç•™0.5ç§’
                            if len(self.audio_buffer) > keep_samples:
                                self.audio_buffer = self.audio_buffer[-keep_samples:]
                    
                    # å¼¹å‡ºå·²å¤„ç†çš„è¯­éŸ³æ®µè½
                    self.vad.pop()
                    
                    print("ğŸ¤ ç»§ç»­ç›‘å¬...")
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"\nå¤„ç†éŸ³é¢‘æ—¶å‡ºé”™: {e}")
                break
    
    def stop_recording(self):
        """åœæ­¢å½•éŸ³"""
        self.is_recording = False
        
        if hasattr(self, 'audio_stream'):
            self.audio_stream.stop_stream()
            self.audio_stream.close()
        
        if hasattr(self, 'audio'):
            self.audio.terminate()
        
        print("\nå½•éŸ³å·²åœæ­¢")
    
    def run_interactive(self, device_id=None):
        """è¿è¡Œäº¤äº’å¼è¯†åˆ«"""
        try:
            # å¼€å§‹å½•éŸ³
            self.start_recording(device_id)
            
            # å¯åŠ¨éŸ³é¢‘å¤„ç†çº¿ç¨‹
            processing_thread = threading.Thread(target=self.process_audio)
            processing_thread.daemon = True
            processing_thread.start()
            
            # ç­‰å¾…ç”¨æˆ·ä¸­æ–­
            while True:
                time.sleep(0.1)
                
        except KeyboardInterrupt:
            print("\n\næ­£åœ¨é€€å‡º...")
        finally:
            self.stop_recording()


def main():
    parser = argparse.ArgumentParser(
        description="åŸºäºSenseVoiceçš„ä¼ªæµå¼è¯­éŸ³è¯†åˆ«æ¼”ç¤º",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python streaming_demo_sensevoice.py
  python streaming_demo_sensevoice.py --model models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17
  python streaming_demo_sensevoice.py --use-gpu --device 0
        """
    )
    
    parser.add_argument(
        "--model",
        type=str,
        default="models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17",
        help="SenseVoiceæ¨¡å‹ç›®å½•è·¯å¾„"
    )
    
    parser.add_argument(
        "--use-gpu",
        action="store_true",
        help="ä½¿ç”¨GPUåŠ é€Ÿ"
    )
    
    parser.add_argument(
        "--device",
        type=int,
        default=None,
        help="æŒ‡å®šéŸ³é¢‘è®¾å¤‡ID"
    )
    
    parser.add_argument(
        "--sample-rate",
        type=int,
        default=16000,
        help="éŸ³é¢‘é‡‡æ ·ç‡ (é»˜è®¤: 16000)"
    )
    
    args = parser.parse_args()
    
    print("åŸºäºSenseVoiceçš„ä¼ªæµå¼è¯­éŸ³è¯†åˆ«æ¼”ç¤º")
    print("=" * 45)
    
    try:
        # åˆ›å»ºä¼ªæµå¼ASRç³»ç»Ÿ
        asr = StreamingSenseVoice(
            model_dir=args.model,
            use_gpu=args.use_gpu,
            sample_rate=args.sample_rate
        )
        
        # è¿è¡Œäº¤äº’å¼è¯†åˆ«
        asr.run_interactive(device_id=args.device)
        
    except FileNotFoundError as e:
        print(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print("è¯·ç¡®ä¿SenseVoiceæ¨¡å‹è·¯å¾„æ­£ç¡®")
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

