"""
ä¼˜åŒ–çš„æ‰¹æ¬¡å¤„ç†æ¨¡å—
å®ç°äºŒé˜¶æ®µå¹¶è¡Œå¤„ç†æ¶æ„ï¼š
1. é˜¶æ®µ1ï¼šASRæ‰¹æ¬¡å¹¶è¡Œå¤„ç†
2. é˜¶æ®µ2ï¼šåå¤„ç†å¹¶è¡Œï¼ˆæ ‡ç‚¹+å£°çº¹ï¼‰
"""

import logging
import time
import asyncio
import psutil
import threading
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import RLock

import numpy as np

from app.config import settings
from app.utils.metrics import MetricsCollector

logger = logging.getLogger(__name__)


@dataclass
class ResourceMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self._cpu_threshold = 80.0  # CPUä½¿ç”¨ç‡é˜ˆå€¼
        self._memory_threshold = 80.0  # å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼
        self._monitoring_enabled = True
    
    def get_cpu_usage(self) -> float:
        """è·å–å½“å‰CPUä½¿ç”¨ç‡"""
        try:
            return psutil.cpu_percent(interval=0.1)
        except Exception as e:
            logger.warning(f"è·å–CPUä½¿ç”¨ç‡å¤±è´¥: {e}")
            return 50.0  # è¿”å›ä¿å®ˆå€¼
    
    def get_memory_usage(self) -> Tuple[float, int, int]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            memory = psutil.virtual_memory()
            return memory.percent, memory.available, memory.total
        except Exception as e:
            logger.warning(f"è·å–å†…å­˜ä½¿ç”¨ç‡å¤±è´¥: {e}")
            return 50.0, 4 * 1024**3, 8 * 1024**3  # è¿”å›ä¿å®ˆå€¼
    
    def should_reduce_concurrency(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥é™ä½å¹¶å‘åº¦"""
        if not self._monitoring_enabled:
            return False
        
        cpu_usage = self.get_cpu_usage()
        memory_percent, _, _ = self.get_memory_usage()
        
        return cpu_usage > self._cpu_threshold or memory_percent > self._memory_threshold
    
    def get_optimal_thread_count(self, base_threads: int) -> int:
        """æ ¹æ®ç³»ç»Ÿèµ„æºè®¡ç®—æœ€ä¼˜çº¿ç¨‹æ•°"""
        if not self._monitoring_enabled:
            return base_threads
        
        cpu_usage = self.get_cpu_usage()
        memory_percent, _, _ = self.get_memory_usage()
        
        # æ ¹æ®CPUå’Œå†…å­˜ä½¿ç”¨ç‡è°ƒæ•´çº¿ç¨‹æ•°
        cpu_factor = max(0.3, 1.0 - (cpu_usage - 50) / 100)
        memory_factor = max(0.3, 1.0 - (memory_percent - 50) / 100)
        
        adjustment_factor = min(cpu_factor, memory_factor)
        optimal_threads = max(1, int(base_threads * adjustment_factor))
        
        if optimal_threads != base_threads:
            logger.info(f"æ ¹æ®èµ„æºä½¿ç”¨ç‡è°ƒæ•´çº¿ç¨‹æ•°: {base_threads} -> {optimal_threads} "
                       f"(CPU: {cpu_usage:.1f}%, å†…å­˜: {memory_percent:.1f}%)")
        
        return optimal_threads


@dataclass
class ErrorRecoveryManager:
    """é”™è¯¯æ¢å¤å’Œé™çº§ç®¡ç†å™¨"""
    
    def __init__(self):
        self.max_retries = 3
        self.retry_delay = 1.0  # ç§’
        self.degradation_threshold = 0.5  # å¤±è´¥ç‡é˜ˆå€¼
        self.failure_history: List[Dict] = []
        self.degradation_active = False
    
    def should_retry(self, attempt: int, exception: Exception) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        if attempt >= self.max_retries:
            return False
        
        # é’ˆå¯¹ä¸åŒç±»å‹çš„å¼‚å¸¸é‡‡ç”¨ä¸åŒç­–ç•¥
        if isinstance(exception, (TimeoutError, asyncio.TimeoutError)):
            return True
        elif isinstance(exception, MemoryError):
            return False  # å†…å­˜é”™è¯¯ä¸é‡è¯•
        elif isinstance(exception, (ConnectionError, OSError)):
            return True
        else:
            return attempt < 2  # å…¶ä»–é”™è¯¯æœ€å¤šé‡è¯•1æ¬¡
    
    def get_retry_delay(self, attempt: int) -> float:
        """è·å–é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ï¼‰"""
        return self.retry_delay * (2 ** attempt)
    
    def should_degrade(self, failure_rate: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¯ç”¨é™çº§å¤„ç†"""
        return failure_rate > self.degradation_threshold
    
    def record_failure(self, operation: str, exception: Exception):
        """è®°å½•å¤±è´¥ä¿¡æ¯"""
        self.failure_history.append({
            'operation': operation,
            'exception': str(exception),
            'timestamp': time.time()
        })
        
        # ä¿æŒæœ€è¿‘çš„100ä¸ªé”™è¯¯è®°å½•
        if len(self.failure_history) > 100:
            self.failure_history = self.failure_history[-100:]
    
    def get_recent_failure_rate(self, window_minutes: int = 5) -> float:
        """è·å–æœ€è¿‘æ—¶é—´çª—å£å†…çš„å¤±è´¥ç‡"""
        if not self.failure_history:
            return 0.0
        
        current_time = time.time()
        window_start = current_time - (window_minutes * 60)
        
        recent_failures = [f for f in self.failure_history if f['timestamp'] >= window_start]
        
        # ç®€åŒ–è®¡ç®—ï¼šå‡è®¾æ¯ä¸ªå¤±è´¥å¯¹åº”10æ¬¡å°è¯•
        total_attempts = len(self.failure_history) * 10
        failure_count = len(recent_failures)
        
        return failure_count / max(total_attempts, 1)


@dataclass
class BatchProcessingConfig:
    """æ‰¹æ¬¡å¤„ç†é…ç½®"""
    enable_optimized_processing: bool = settings.ENABLE_OPTIMIZED_BATCH_PROCESSING
    enable_parallel_post_processing: bool = settings.ENABLE_PARALLEL_POST_PROCESSING
    
    # é˜¶æ®µ1ï¼šASRæ‰¹æ¬¡é…ç½®
    max_batch_threads: int = settings.MAX_BATCH_THREADS
    min_batch_size: int = settings.MIN_BATCH_SIZE
    max_batch_size: int = settings.MAX_BATCH_SIZE
    asr_threads_per_batch: int = settings.ASR_THREADS_PER_BATCH
    
    # é˜¶æ®µ2ï¼šåå¤„ç†å¹¶è¡Œé…ç½®
    punctuation_threads_per_batch: int = settings.PUNCTUATION_THREADS_PER_BATCH
    speaker_threads_per_batch: int = settings.SPEAKER_THREADS_PER_BATCH
    post_processing_batch_size: int = settings.POST_PROCESSING_BATCH_SIZE
    post_processing_timeout: int = settings.POST_PROCESSING_TIMEOUT
    
    def validate(self) -> bool:
        """éªŒè¯é…ç½®å‚æ•°"""
        errors = []
        
        if self.max_batch_threads < 1:
            errors.append(f"max_batch_threadså¿…é¡»è‡³å°‘ä¸º1: {self.max_batch_threads}")
        
        if self.min_batch_size < 1:
            errors.append(f"min_batch_sizeå¿…é¡»è‡³å°‘ä¸º1: {self.min_batch_size}")
        
        if self.max_batch_size < self.min_batch_size:
            errors.append(f"max_batch_sizeå¿…é¡»å¤§äºç­‰äºmin_batch_size: {self.max_batch_size} < {self.min_batch_size}")
        
        if self.post_processing_batch_size < 1:
            errors.append(f"post_processing_batch_sizeå¿…é¡»è‡³å°‘ä¸º1: {self.post_processing_batch_size}")
        
        if self.post_processing_timeout < 1:
            errors.append(f"post_processing_timeoutå¿…é¡»è‡³å°‘ä¸º1: {self.post_processing_timeout}")
        
        if errors:
            logger.error(f"æ‰¹æ¬¡å¤„ç†é…ç½®éªŒè¯å¤±è´¥: {'; '.join(errors)}")
            raise ValueError(f"æ‰¹æ¬¡å¤„ç†é…ç½®é”™è¯¯: {'; '.join(errors)}")
        
        return True


@dataclass
class BatchProcessingStats:
    """æ‰¹æ¬¡å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    total_segments_processed: int = 0
    total_processing_time: float = 0.0
    stage1_time: float = 0.0  # ASRé˜¶æ®µæ—¶é—´
    stage2_time: float = 0.0  # åå¤„ç†é˜¶æ®µæ—¶é—´
    batches_created: int = 0
    batches_completed: int = 0
    batches_failed: int = 0
    batches_retried: int = 0  # é‡è¯•çš„æ‰¹æ¬¡æ•°
    partial_failures: int = 0  # éƒ¨åˆ†å¤±è´¥çš„å¤„ç†æ•°
    degraded_processing: int = 0  # é™çº§å¤„ç†æ¬¡æ•°
    parallel_efficiency: float = 0.0  # å¹¶è¡Œæ•ˆç‡
    error_recovery_success: int = 0  # é”™è¯¯æ¢å¤æˆåŠŸæ¬¡æ•°
    
    def update_stage1_stats(self, processing_time: float, segments_count: int, batches_count: int):
        """æ›´æ–°é˜¶æ®µ1ç»Ÿè®¡"""
        self.stage1_time += processing_time
        self.total_segments_processed += segments_count
        self.batches_created += batches_count
    
    def update_stage2_stats(self, processing_time: float):
        """æ›´æ–°é˜¶æ®µ2ç»Ÿè®¡"""
        self.stage2_time += processing_time
    
    def update_completion_stats(self, completed: int, failed: int):
        """æ›´æ–°å®Œæˆç»Ÿè®¡"""
        self.batches_completed += completed
        self.batches_failed += failed
        self.total_processing_time = self.stage1_time + self.stage2_time
        
        # è®¡ç®—å¹¶è¡Œæ•ˆç‡
        if self.total_processing_time > 0:
            theoretical_sequential_time = self.stage1_time + self.stage2_time
            self.parallel_efficiency = theoretical_sequential_time / self.total_processing_time


@dataclass
class ProcessingSegment:
    """å¤„ç†æ®µè½æ•°æ®ç»“æ„"""
    index: int
    audio_samples: np.ndarray
    sample_rate: int
    start_time: float
    end_time: float
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class ASRResult:
    """ASRç»“æœæ•°æ®ç»“æ„"""
    index: int
    text: str
    confidence: float
    start_time: float
    end_time: float
    language: str = "unknown"
    emotion: str = "unknown"
    event: str = "unknown"


@dataclass
class PostProcessingResult:
    """åå¤„ç†ç»“æœæ•°æ®ç»“æ„"""
    index: int
    text_with_punct: str = ""
    speaker_info: str = "unknown"
    speaker_confidence: float = 0.0


class OptimizedBatchProcessor:
    """ä¼˜åŒ–çš„æ‰¹æ¬¡å¤„ç†å™¨ - äºŒé˜¶æ®µå¹¶è¡Œå¤„ç†"""
    
    def __init__(self, config: Optional[BatchProcessingConfig] = None):
        self.config = config or BatchProcessingConfig()
        self.config.validate()
        
        self.stats = BatchProcessingStats()
        self._lock = RLock()
        
        # èµ„æºç›‘æ§å™¨
        self.resource_monitor = ResourceMonitor()
        
        # é”™è¯¯æ¢å¤ç®¡ç†å™¨
        self.error_recovery = ErrorRecoveryManager()
        
        # ä¸“ç”¨çº¿ç¨‹æ± ç®¡ç†
        self._asr_thread_pool: Optional[ThreadPoolExecutor] = None
        self._punctuation_thread_pool: Optional[ThreadPoolExecutor] = None
        self._speaker_thread_pool: Optional[ThreadPoolExecutor] = None
        self._thread_pools_initialized = False
        
        # æ€§èƒ½ç›‘æ§
        self.metrics_collector: Optional[MetricsCollector] = None
        if hasattr(settings, 'ENABLE_METRICS') and settings.ENABLE_METRICS:
            try:
                self.metrics_collector = MetricsCollector()
            except Exception as e:
                logger.warning(f"æ— æ³•åˆå§‹åŒ–æ€§èƒ½ç›‘æ§: {e}")
        
        logger.info(f"ä¼˜åŒ–æ‰¹æ¬¡å¤„ç†å™¨åˆ›å»ºå®Œæˆï¼Œé…ç½®: {asdict(self.config)}")
    
    def _initialize_thread_pools(self):
        """åˆå§‹åŒ–ä¸“ç”¨çº¿ç¨‹æ± """
        if self._thread_pools_initialized:
            return
        
        with self._lock:
            if self._thread_pools_initialized:
                return
            
            # æ ¹æ®ç³»ç»Ÿèµ„æºåŠ¨æ€è°ƒæ•´çº¿ç¨‹æ± å¤§å°
            base_asr_threads = self.config.max_batch_threads
            base_post_threads = (self.config.punctuation_threads_per_batch + 
                               self.config.speaker_threads_per_batch)
            
            optimal_asr_threads = self.resource_monitor.get_optimal_thread_count(base_asr_threads)
            optimal_post_threads = self.resource_monitor.get_optimal_thread_count(base_post_threads)
            
            # åˆ›å»ºASRä¸“ç”¨çº¿ç¨‹æ± 
            self._asr_thread_pool = ThreadPoolExecutor(
                max_workers=optimal_asr_threads,
                thread_name_prefix="asr_batch"
            )
            
            # åˆ›å»ºåå¤„ç†ä¸“ç”¨çº¿ç¨‹æ± 
            self._post_processing_thread_pool = ThreadPoolExecutor(
                max_workers=optimal_post_threads,
                thread_name_prefix="post_processing"
            )
            
            self._thread_pools_initialized = True
            logger.info(f"çº¿ç¨‹æ± åˆå§‹åŒ–å®Œæˆ - ASR: {optimal_asr_threads} çº¿ç¨‹, åå¤„ç†: {optimal_post_threads} çº¿ç¨‹")
    
    def _cleanup_thread_pools(self):
        """æ¸…ç†çº¿ç¨‹æ± èµ„æº"""
        with self._lock:
            if self._asr_thread_pool:
                self._asr_thread_pool.shutdown(wait=True)
                self._asr_thread_pool = None
            
            if self._post_processing_thread_pool:
                self._post_processing_thread_pool.shutdown(wait=True)
                self._post_processing_thread_pool = None
            
            self._thread_pools_initialized = False
            logger.info("çº¿ç¨‹æ± èµ„æºå·²æ¸…ç†")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        try:
            self._cleanup_thread_pools()
        except Exception as e:
            logger.warning(f"æ¸…ç†çº¿ç¨‹æ± èµ„æºæ—¶å‡ºé”™: {e}")
    
    async def process_segments_optimized(
        self,
        segments: List[Dict[str, Any]],
        enable_punctuation: bool = True,
        enable_speaker_id: bool = True,
        asr_model=None,
        punctuation_processor=None,
        speaker_extractor=None
    ) -> List[Dict[str, Any]]:
        """
        ä¼˜åŒ–çš„æ®µè½å¤„ç†æµç¨‹
        
        é˜¶æ®µ1ï¼šæ‰¹æ¬¡çº§ASRå¹¶è¡Œ
        é˜¶æ®µ2ï¼šåå¤„ç†å¹¶è¡Œ (æ ‡ç‚¹ + å£°çº¹)
        """
        if not segments:
            return []
        
        total_start_time = time.time()
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ä¼˜åŒ–å¤„ç†
        if not self.config.enable_optimized_processing:
            logger.info("ä½¿ç”¨ä¼ ç»Ÿæ‰¹æ¬¡å¤„ç†ï¼ˆä¼˜åŒ–å¤„ç†å·²ç¦ç”¨ï¼‰")
            return await self._fallback_processing(segments, enable_punctuation, enable_speaker_id, asr_model, punctuation_processor, speaker_extractor)
        
        # åˆå§‹åŒ–çº¿ç¨‹æ± 
        self._initialize_thread_pools()
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æºå¹¶è°ƒæ•´å¤„ç†ç­–ç•¥
        if self.resource_monitor.should_reduce_concurrency():
            logger.warning("ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå°†é™ä½å¹¶å‘å¤„ç†å¼ºåº¦")
        
        logger.info(f"ğŸš€ å¼€å§‹ä¼˜åŒ–æ‰¹æ¬¡å¤„ç†ï¼Œå…± {len(segments)} ä¸ªæ®µè½")
        cpu_usage = self.resource_monitor.get_cpu_usage()
        memory_percent, memory_available, memory_total = self.resource_monitor.get_memory_usage()
        logger.info(f"ğŸ“Š ç³»ç»Ÿèµ„æºçŠ¶æ€ - CPU: {cpu_usage:.1f}%, å†…å­˜: {memory_percent:.1f}% ({memory_available//1024**2}MB å¯ç”¨)")
        
        try:
            # å‡†å¤‡å¤„ç†æ®µè½
            processing_segments = self._prepare_segments(segments)
            
            # é˜¶æ®µ1ï¼šASRæ‰¹æ¬¡å¹¶è¡Œå¤„ç†
            asr_results = await self._stage1_parallel_asr_processing(processing_segments, asr_model)
            
            # é˜¶æ®µ2ï¼šåå¤„ç†å¹¶è¡Œ
            if self.config.enable_parallel_post_processing:
                final_results = await self._stage2_parallel_post_processing(
                    asr_results, processing_segments, enable_punctuation, enable_speaker_id,
                    punctuation_processor, speaker_extractor
                )
            else:
                final_results = await self._stage2_sequential_post_processing(
                    asr_results, processing_segments, enable_punctuation, enable_speaker_id,
                    punctuation_processor, speaker_extractor
                )
            
            # æ›´æ–°ç»Ÿè®¡
            total_time = time.time() - total_start_time
            self.stats.update_completion_stats(len(final_results), 0)
            
            # æ€§èƒ½ç›‘æ§
            if self.metrics_collector:
                self.metrics_collector.record_processing_time("optimized_batch_processing", total_time)
                self.metrics_collector.record_counter("segments_processed", len(segments))
            
            logger.info(f"ğŸ‰ ä¼˜åŒ–æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’ï¼Œå¹³å‡æ¯æ®µ: {total_time/len(segments):.3f}ç§’")
            logger.info(f"ğŸ“Š é˜¶æ®µ1è€—æ—¶: {self.stats.stage1_time:.2f}ç§’ï¼Œé˜¶æ®µ2è€—æ—¶: {self.stats.stage2_time:.2f}ç§’")
            logger.info(f"âš¡ å¹¶è¡Œæ•ˆç‡: {self.stats.parallel_efficiency:.2%}")
            
            return final_results
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            self.error_recovery.record_failure("optimized_batch_processing", e)
            self.stats.batches_failed += 1
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¯ç”¨é™çº§å¤„ç†
            failure_rate = self.error_recovery.get_recent_failure_rate()
            if self.error_recovery.should_degrade(failure_rate):
                logger.warning(f"å¤±è´¥ç‡è¿‡é«˜ ({failure_rate:.2%})ï¼Œå°†å¯ç”¨æ›´ä¿å®ˆçš„é™çº§å¤„ç†ç­–ç•¥")
                self.stats.degraded_processing += 1
            
            # é™çº§åˆ°ä¼ ç»Ÿå¤„ç†
            logger.info("é™çº§åˆ°ä¼ ç»Ÿæ‰¹æ¬¡å¤„ç†æ¨¡å¼")
            return await self._fallback_processing(segments, enable_punctuation, enable_speaker_id, asr_model, punctuation_processor, speaker_extractor)
    
    def _prepare_segments(self, segments: List[Dict[str, Any]]) -> List[ProcessingSegment]:
        """å‡†å¤‡å¤„ç†æ®µè½"""
        processing_segments = []
        
        for i, segment in enumerate(segments):
            processing_segment = ProcessingSegment(
                index=i,
                audio_samples=segment['samples'],
                sample_rate=segment['sample_rate'],
                start_time=segment['start_time'],
                end_time=segment['end_time'],
                metadata=segment.get('metadata', {})
            )
            processing_segments.append(processing_segment)
        
        return processing_segments
    
    async def _stage1_parallel_asr_processing(
        self,
        segments: List[ProcessingSegment],
        asr_model
    ) -> List[ASRResult]:
        """é˜¶æ®µ1ï¼šASRæ‰¹æ¬¡å¹¶è¡Œå¤„ç†"""
        if not asr_model:
            logger.error("ASRæ¨¡å‹æœªæä¾›ï¼Œæ— æ³•è¿›è¡Œè¯­éŸ³è¯†åˆ«")
            return []
        
        stage1_start_time = time.time()
        logger.info(f"ğŸ¯ é˜¶æ®µ1ï¼šASRæ‰¹æ¬¡å¹¶è¡Œå¤„ç†å¼€å§‹ï¼Œ{len(segments)} ä¸ªæ®µè½")
        
        # è®¡ç®—æ‰¹æ¬¡åˆ’åˆ†
        batch_strategy = self._calculate_batch_strategy(len(segments))
        batches = self._create_batches(segments, batch_strategy['batch_size'])
        
        logger.info(f"ğŸ“¦ åˆ›å»º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_strategy['batch_size']} ä¸ªæ®µè½ï¼Œä½¿ç”¨ {batch_strategy['max_workers']} ä¸ªçº¿ç¨‹")
        
        # ç¡®ä¿çº¿ç¨‹æ± å·²åˆå§‹åŒ–
        if not self._thread_pools_initialized:
            self._initialize_thread_pools()
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
        loop = asyncio.get_event_loop()
        batch_futures = []
        
        for batch_idx, batch in enumerate(batches):
            future = loop.run_in_executor(
                self._asr_thread_pool,  # ä½¿ç”¨ä¸“ç”¨ASRçº¿ç¨‹æ± 
                self._process_asr_batch,
                batch,
                batch_idx + 1,
                len(batches),
                asr_model
            )
            batch_futures.append(future)
        
        # ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ
        batch_results = await asyncio.gather(*batch_futures, return_exceptions=True)
        
        # æ”¶é›†ç»“æœå¹¶å¤„ç†å¤±è´¥çš„æ‰¹æ¬¡
        all_asr_results = []
        successful_batches = 0
        failed_batches = 0
        retry_batches = []
        
        for i, result in enumerate(batch_results):
            if isinstance(result, Exception):
                logger.error(f"ASRæ‰¹æ¬¡ {i+1} å¤„ç†å¤±è´¥: {result}")
                self.error_recovery.record_failure(f"asr_batch_{i+1}", result)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if self.error_recovery.should_retry(0, result):
                    retry_batches.append((i, batches[i]))
                    logger.info(f"å°†é‡è¯•ASRæ‰¹æ¬¡ {i+1}")
                else:
                    failed_batches += 1
            else:
                all_asr_results.extend(result)
                successful_batches += 1
        
        # å¤„ç†éœ€è¦é‡è¯•çš„æ‰¹æ¬¡
        if retry_batches:
            logger.info(f"å¼€å§‹é‡è¯• {len(retry_batches)} ä¸ªå¤±è´¥çš„ASRæ‰¹æ¬¡")
            retry_results = await self._retry_failed_batches(retry_batches, asr_model)
            all_asr_results.extend(retry_results)
            self.stats.batches_retried += len(retry_batches)
        
        # æŒ‰ç´¢å¼•æ’åºç»“æœ
        all_asr_results.sort(key=lambda x: x.index)
        
        stage1_time = time.time() - stage1_start_time
        self.stats.update_stage1_stats(stage1_time, len(segments), len(batches))
        
        logger.info(f"âœ… é˜¶æ®µ1å®Œæˆï¼Œè€—æ—¶: {stage1_time:.2f}ç§’ï¼ŒæˆåŠŸæ‰¹æ¬¡: {successful_batches}/{len(batches)}")
        
        return all_asr_results
    
    async def _retry_failed_batches(
        self,
        retry_batches: List[Tuple[int, List[ProcessingSegment]]],
        asr_model
    ) -> List[ASRResult]:
        """é‡è¯•å¤±è´¥çš„ASRæ‰¹æ¬¡"""
        retry_results = []
        
        for batch_idx, batch_segments in retry_batches:
            for attempt in range(1, self.error_recovery.max_retries + 1):
                try:
                    # æ·»åŠ é‡è¯•å»¶è¿Ÿ
                    if attempt > 1:
                        delay = self.error_recovery.get_retry_delay(attempt - 1)
                        await asyncio.sleep(delay)
                        logger.info(f"é‡è¯•ASRæ‰¹æ¬¡ {batch_idx + 1}ï¼Œç¬¬ {attempt} æ¬¡å°è¯•ï¼ˆå»¶è¿Ÿ {delay:.1f}ç§’ï¼‰")
                    
                    # å°è¯•é‡æ–°å¤„ç†æ‰¹æ¬¡
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(
                        self._asr_thread_pool,
                        self._process_asr_batch,
                        batch_segments,
                        batch_idx + 1,
                        1,  # é‡è¯•æ—¶åªæœ‰è¿™ä¸€ä¸ªæ‰¹æ¬¡
                        asr_model
                    )
                    
                    retry_results.extend(result)
                    self.stats.error_recovery_success += 1
                    logger.info(f"ASRæ‰¹æ¬¡ {batch_idx + 1} é‡è¯•æˆåŠŸ")
                    break
                    
                except Exception as e:
                    logger.warning(f"ASRæ‰¹æ¬¡ {batch_idx + 1} ç¬¬ {attempt} æ¬¡é‡è¯•å¤±è´¥: {e}")
                    self.error_recovery.record_failure(f"asr_batch_{batch_idx + 1}_retry_{attempt}", e)
                    
                    if attempt == self.error_recovery.max_retries:
                        logger.error(f"ASRæ‰¹æ¬¡ {batch_idx + 1} é‡è¯•æ¬¡æ•°å·²ç”¨å®Œï¼Œæ”¾å¼ƒå¤„ç†")
                        # ä¸ºå¤±è´¥çš„æ®µè½åˆ›å»ºç©ºç»“æœ
                        for segment in batch_segments:
                            error_result = ASRResult(
                                index=segment.index,
                                text="",
                                confidence=0.0,
                                start_time=segment.start_time,
                                end_time=segment.end_time
                            )
                            retry_results.append(error_result)
        
        return retry_results
    
    def _calculate_batch_strategy(self, total_segments: int) -> Dict[str, int]:
        """è®¡ç®—æ‰¹æ¬¡åˆ’åˆ†ç­–ç•¥ï¼Œè€ƒè™‘ç³»ç»Ÿèµ„æºçº¦æŸå’Œé”™è¯¯å†å²"""
        if total_segments <= 10:
            return {
                'batch_size': total_segments,
                'max_workers': 1,
                'num_batches': 1
            }
        
        # æ ¹æ®ç³»ç»Ÿèµ„æºåŠ¨æ€è°ƒæ•´åŸºç¡€é…ç½®
        base_max_threads = self.config.max_batch_threads
        optimal_max_threads = self.resource_monitor.get_optimal_thread_count(base_max_threads)
        
        # æ£€æŸ¥é”™è¯¯å†å²ï¼Œå¦‚æœæœ€è¿‘å¤±è´¥ç‡è¾ƒé«˜ï¼Œå¯ç”¨ä¿å®ˆç­–ç•¥
        failure_rate = self.error_recovery.get_recent_failure_rate()
        conservative_mode = self.error_recovery.should_degrade(failure_rate)
        
        if conservative_mode:
            optimal_max_threads = max(1, optimal_max_threads // 2)
            self.stats.degraded_processing += 1
            logger.warning(f"æ£€æµ‹åˆ°é«˜å¤±è´¥ç‡ ({failure_rate:.2%})ï¼Œå¯ç”¨ä¿å®ˆå¤„ç†æ¨¡å¼")
        
        # æ ¹æ®é…ç½®å’Œèµ„æºçº¦æŸè®¡ç®—æ‰¹æ¬¡å¤§å°
        batch_size = max(
            self.config.min_batch_size,
            min(self.config.max_batch_size, total_segments // optimal_max_threads)
        )
        
        # å¦‚æœç³»ç»Ÿèµ„æºç´§å¼ ï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°ä»¥å‡å°‘å¹¶å‘åº¦
        if self.resource_monitor.should_reduce_concurrency():
            batch_size = min(self.config.max_batch_size, batch_size * 2)
            logger.info(f"ç³»ç»Ÿèµ„æºç´§å¼ ï¼Œè°ƒæ•´æ‰¹æ¬¡å¤§å°ä¸º: {batch_size}")
        
        # å¦‚æœæ˜¯ä¿å®ˆæ¨¡å¼ï¼Œè¿›ä¸€æ­¥å¢åŠ æ‰¹æ¬¡å¤§å°
        if conservative_mode:
            batch_size = min(self.config.max_batch_size, batch_size * 2)
            logger.info(f"ä¿å®ˆæ¨¡å¼ä¸‹ï¼Œè¿›ä¸€æ­¥è°ƒæ•´æ‰¹æ¬¡å¤§å°ä¸º: {batch_size}")
        
        # è®¡ç®—æ‰¹æ¬¡æ•°é‡
        num_batches = (total_segments + batch_size - 1) // batch_size
        
        # æ ¹æ®æ‰¹æ¬¡æ•°é‡å’Œèµ„æºçº¦æŸè°ƒæ•´çº¿ç¨‹æ•°
        max_workers = min(optimal_max_threads, num_batches)
        
        return {
            'batch_size': batch_size,
            'max_workers': max_workers,
            'num_batches': num_batches
        }
    
    def _create_batches(self, segments: List[ProcessingSegment], batch_size: int) -> List[List[ProcessingSegment]]:
        """åˆ›å»ºæ‰¹æ¬¡"""
        batches = []
        for i in range(0, len(segments), batch_size):
            batch = segments[i:i + batch_size]
            batches.append(batch)
        return batches
    
    def _process_asr_batch(
        self,
        batch_segments: List[ProcessingSegment],
        batch_idx: int,
        total_batches: int,
        asr_model
    ) -> List[ASRResult]:
        """å¤„ç†å•ä¸ªASRæ‰¹æ¬¡"""
        batch_start_time = time.time()
        logger.info(f"ğŸ”„ ASRæ‰¹æ¬¡ {batch_idx}/{total_batches} å¼€å§‹ï¼ŒåŒ…å« {len(batch_segments)} ä¸ªæ®µè½")
        
        try:
            # åˆ›å»ºè¯†åˆ«æµ
            streams = []
            for segment in batch_segments:
                stream = asr_model.offline_recognizer.create_stream()
                stream.accept_waveform(segment.sample_rate, segment.audio_samples)
                streams.append(stream)
            
            # æ‰¹é‡è¯†åˆ«
            asr_model.offline_recognizer.decode_streams(streams)
            
            # æ”¶é›†ç»“æœ
            batch_results = []
            for i, stream in enumerate(streams):
                result = stream.result
                segment = batch_segments[i]
                
                asr_result = ASRResult(
                    index=segment.index,
                    text=result.text,
                    confidence=getattr(result, 'confidence', 0.0),
                    start_time=segment.start_time,
                    end_time=segment.end_time,
                    language=getattr(result, 'lang', 'unknown'),
                    emotion=getattr(result, 'emotion', 'unknown'),
                    event=getattr(result, 'event', 'unknown')
                )
                batch_results.append(asr_result)
            
            batch_time = time.time() - batch_start_time
            logger.info(f"âœ… ASRæ‰¹æ¬¡ {batch_idx} å®Œæˆï¼Œè€—æ—¶: {batch_time:.2f}ç§’")
            
            return batch_results
            
        except Exception as e:
            logger.error(f"ASRæ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯å ä½ç»“æœ
            error_results = []
            for segment in batch_segments:
                error_result = ASRResult(
                    index=segment.index,
                    text="",
                    confidence=0.0,
                    start_time=segment.start_time,
                    end_time=segment.end_time
                )
                error_results.append(error_result)
            return error_results
    
    async def _stage2_parallel_post_processing(
        self,
        asr_results: List[ASRResult],
        segments: List[ProcessingSegment],
        enable_punctuation: bool,
        enable_speaker_id: bool,
        punctuation_processor,
        speaker_extractor
    ) -> List[Dict[str, Any]]:
        """é˜¶æ®µ2ï¼šåå¤„ç†å¹¶è¡Œï¼ˆæ ‡ç‚¹+å£°çº¹ï¼‰"""
        stage2_start_time = time.time()
        logger.info(f"ğŸ¯ é˜¶æ®µ2ï¼šåå¤„ç†å¹¶è¡Œå¼€å§‹ï¼Œå¤„ç† {len(asr_results)} ä¸ªç»“æœ")
        
        # ç¡®ä¿çº¿ç¨‹æ± å·²åˆå§‹åŒ–
        if not self._thread_pools_initialized:
            self._initialize_thread_pools()
        
        # ä½¿ç”¨ä¸“ç”¨åå¤„ç†çº¿ç¨‹æ± 
        executor = self._post_processing_thread_pool
        
        # æäº¤æ ‡ç‚¹å¤„ç†ä»»åŠ¡
        punctuation_futures = {}
        if enable_punctuation and punctuation_processor:
            for asr_result in asr_results:
                if asr_result.text.strip():
                    future = executor.submit(
                        self._process_punctuation_single,
                        asr_result.text,
                        asr_result.index,
                        punctuation_processor
                    )
                    punctuation_futures[future] = asr_result.index
        
        # æäº¤å£°çº¹è¯†åˆ«ä»»åŠ¡
        speaker_futures = {}
        if enable_speaker_id and speaker_extractor:
            for i, segment in enumerate(segments):
                future = executor.submit(
                    self._process_speaker_single,
                    segment.audio_samples,
                    segment.sample_rate,
                    segment.index,
                    speaker_extractor
                )
                speaker_futures[future] = segment.index
        
        # æ”¶é›†å¹¶è¡Œç»“æœ
        punctuation_results = {}
        speaker_results = {}
        
        # ç­‰å¾…æ ‡ç‚¹å¤„ç†å®Œæˆï¼Œå¢åŠ é”™è¯¯è®¡æ•°å’Œè¶…æ—¶å¤„ç†
        punctuation_success = 0
        punctuation_failures = 0
        
        try:
            for future in as_completed(punctuation_futures, timeout=self.config.post_processing_timeout):
                try:
                    punctuated_text, index = future.result()
                    punctuation_results[index] = punctuated_text
                    punctuation_success += 1
                except Exception as e:
                    index = punctuation_futures[future]
                    logger.warning(f"æ ‡ç‚¹å¤„ç†å¤±è´¥ (æ®µè½ {index}): {e}")
                    self.error_recovery.record_failure(f"punctuation_{index}", e)
                    punctuation_failures += 1
                    # ä½¿ç”¨åŸå§‹æ–‡æœ¬ä½œä¸ºé™çº§æ–¹æ¡ˆ
                    original_text = next((r.text for r in asr_results if r.index == index), "")
                    punctuation_results[index] = original_text
        except asyncio.TimeoutError:
            logger.error(f"æ ‡ç‚¹å¤„ç†è¶…æ—¶ï¼Œå·²å¤„ç†: {punctuation_success}ï¼Œå¤±è´¥: {punctuation_failures}")
            self.stats.partial_failures += 1
            # ä¸ºè¶…æ—¶çš„ä»»åŠ¡æä¾›é™çº§ç»“æœ
            for future, index in punctuation_futures.items():
                if index not in punctuation_results:
                    original_text = next((r.text for r in asr_results if r.index == index), "")
                    punctuation_results[index] = original_text
        
        # ç­‰å¾…å£°çº¹è¯†åˆ«å®Œæˆï¼Œå¢åŠ é”™è¯¯è®¡æ•°å’Œè¶…æ—¶å¤„ç†
        speaker_success = 0
        speaker_failures = 0
        
        try:
            for future in as_completed(speaker_futures, timeout=self.config.post_processing_timeout):
                try:
                    speaker_info, confidence, index = future.result()
                    speaker_results[index] = {
                        'speaker': speaker_info,
                        'confidence': confidence
                    }
                    speaker_success += 1
                except Exception as e:
                    index = speaker_futures[future]
                    logger.warning(f"å£°çº¹è¯†åˆ«å¤±è´¥ (æ®µè½ {index}): {e}")
                    self.error_recovery.record_failure(f"speaker_id_{index}", e)
                    speaker_failures += 1
                    speaker_results[index] = {
                        'speaker': 'unknown',
                        'confidence': 0.0
                    }
        except asyncio.TimeoutError:
            logger.error(f"å£°çº¹è¯†åˆ«è¶…æ—¶ï¼Œå·²å¤„ç†: {speaker_success}ï¼Œå¤±è´¥: {speaker_failures}")
            self.stats.partial_failures += 1
            # ä¸ºè¶…æ—¶çš„ä»»åŠ¡æä¾›é™çº§ç»“æœ
            for future, index in speaker_futures.items():
                if index not in speaker_results:
                    speaker_results[index] = {
                        'speaker': 'unknown',
                        'confidence': 0.0
                    }
        
        # åˆå¹¶æœ€ç»ˆç»“æœ
        final_results = []
        for asr_result in asr_results:
            final_result = {
                'text': asr_result.text,
                'start_time': asr_result.start_time,
                'end_time': asr_result.end_time,
                'language': asr_result.language,
                'emotion': asr_result.emotion,
                'event': asr_result.event,
                'confidence': asr_result.confidence
            }
            
            # æ·»åŠ æ ‡ç‚¹ç»“æœ
            if asr_result.index in punctuation_results:
                final_result['text_with_punct'] = punctuation_results[asr_result.index]
                final_result['text'] = punctuation_results[asr_result.index]
            else:
                final_result['text_with_punct'] = asr_result.text
            
            # æ·»åŠ å£°çº¹ç»“æœ
            if asr_result.index in speaker_results:
                final_result['speaker'] = speaker_results[asr_result.index]['speaker']
                final_result['speaker_confidence'] = speaker_results[asr_result.index]['confidence']
            else:
                final_result['speaker'] = 'unknown'
                final_result['speaker_confidence'] = 0.0
            
            final_results.append(final_result)
        
        stage2_time = time.time() - stage2_start_time
        self.stats.update_stage2_stats(stage2_time)
        
        logger.info(f"âœ… é˜¶æ®µ2å®Œæˆï¼Œè€—æ—¶: {stage2_time:.2f}ç§’")
        logger.info(f"ğŸ“ˆ æ ‡ç‚¹å¤„ç†: {len(punctuation_results)}/{len(asr_results)}, å£°çº¹è¯†åˆ«: {len(speaker_results)}/{len(segments)}")
        
        return final_results
    
    def _process_punctuation_single(self, text: str, index: int, punctuation_processor) -> Tuple[str, int]:
        """å¤„ç†å•ä¸ªæ®µè½çš„æ ‡ç‚¹"""
        try:
            punctuated_text = punctuation_processor.add_punctuation(text)
            return punctuated_text, index
        except Exception as e:
            logger.warning(f"æ ‡ç‚¹å¤„ç†å¤±è´¥ (æ®µè½ {index}): {e}")
            return text, index
    
    def _process_speaker_single(self, audio_samples: np.ndarray, sample_rate: int, index: int, speaker_extractor) -> Tuple[str, float, int]:
        """å¤„ç†å•ä¸ªæ®µè½çš„å£°çº¹è¯†åˆ«"""
        try:
            # æå–è¯´è¯äººåµŒå…¥
            stream = speaker_extractor.create_stream()
            stream.accept_waveform(sample_rate, audio_samples)
            stream.input_finished()
            
            embedding = speaker_extractor.compute(stream)
            if isinstance(embedding, list):
                embedding = np.array(embedding)
            
            # ç®€åŒ–çš„è¯´è¯äººè¯†åˆ«é€»è¾‘ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨SpeakerManagerï¼‰
            speaker_id = f"Speaker_{hash(tuple(embedding.tolist()[:10])) % 1000:03d}"
            confidence = 0.8  # ç®€åŒ–çš„ç½®ä¿¡åº¦
            
            return speaker_id, confidence, index
            
        except Exception as e:
            logger.warning(f"å£°çº¹è¯†åˆ«å¤±è´¥ (æ®µè½ {index}): {e}")
            return 'unknown', 0.0, index
    
    async def _stage2_sequential_post_processing(
        self,
        asr_results: List[ASRResult],
        segments: List[ProcessingSegment],
        enable_punctuation: bool,
        enable_speaker_id: bool,
        punctuation_processor,
        speaker_extractor
    ) -> List[Dict[str, Any]]:
        """é˜¶æ®µ2ï¼šé¡ºåºåå¤„ç†ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        logger.info("ğŸ”„ ä½¿ç”¨é¡ºåºåå¤„ç†ï¼ˆå¹¶è¡Œå¤„ç†å·²ç¦ç”¨ï¼‰")
        
        final_results = []
        for i, asr_result in enumerate(asr_results):
            result = {
                'text': asr_result.text,
                'start_time': asr_result.start_time,
                'end_time': asr_result.end_time,
                'language': asr_result.language,
                'emotion': asr_result.emotion,
                'event': asr_result.event,
                'confidence': asr_result.confidence
            }
            
            # æ ‡ç‚¹å¤„ç†
            if enable_punctuation and punctuation_processor and asr_result.text.strip():
                try:
                    result['text_with_punct'] = punctuation_processor.add_punctuation(asr_result.text)
                    result['text'] = result['text_with_punct']
                except Exception as e:
                    logger.warning(f"æ ‡ç‚¹å¤„ç†å¤±è´¥: {e}")
                    result['text_with_punct'] = asr_result.text
            else:
                result['text_with_punct'] = asr_result.text
            
            # å£°çº¹è¯†åˆ«
            if enable_speaker_id and speaker_extractor and i < len(segments):
                try:
                    speaker_info, confidence, _ = self._process_speaker_single(
                        segments[i].audio_samples, segments[i].sample_rate, i, speaker_extractor
                    )
                    result['speaker'] = speaker_info
                    result['speaker_confidence'] = confidence
                except Exception as e:
                    logger.warning(f"å£°çº¹è¯†åˆ«å¤±è´¥: {e}")
                    result['speaker'] = 'unknown'
                    result['speaker_confidence'] = 0.0
            else:
                result['speaker'] = 'unknown'
                result['speaker_confidence'] = 0.0
            
            final_results.append(result)
        
        return final_results
    
    async def _fallback_processing(
        self,
        segments: List[Dict[str, Any]],
        enable_punctuation: bool,
        enable_speaker_id: bool,
        asr_model,
        punctuation_processor,
        speaker_extractor
    ) -> List[Dict[str, Any]]:
        """é™çº§åˆ°ä¼ ç»Ÿæ‰¹æ¬¡å¤„ç†"""
        logger.info("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿæ‰¹æ¬¡å¤„ç†ï¼ˆé™çº§æ¨¡å¼ï¼‰")
        
        if asr_model and hasattr(asr_model, '_parallel_recognize_segments'):
            return await asr_model._parallel_recognize_segments(
                segments, enable_speaker_id, enable_punctuation
            )
        else:
            # æœ€åŸºç¡€çš„é¡ºåºå¤„ç†
            results = []
            for segment in segments:
                result = {
                    'text': '',
                    'start_time': segment['start_time'],
                    'end_time': segment['end_time'],
                    'language': 'unknown',
                    'emotion': 'unknown',
                    'event': 'unknown',
                    'speaker': 'unknown',
                    'text_with_punct': '',
                    'confidence': 0.0,
                    'speaker_confidence': 0.0
                }
                results.append(result)
            return results
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats_dict = asdict(self.stats)
        stats_dict['config'] = asdict(self.config)
        return stats_dict
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = BatchProcessingStats()
        logger.info("æ‰¹æ¬¡å¤„ç†ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")


# å…¨å±€æ‰¹æ¬¡å¤„ç†å™¨å®ä¾‹
_global_batch_processor: Optional[OptimizedBatchProcessor] = None

async def get_batch_processor(config: Optional[BatchProcessingConfig] = None) -> OptimizedBatchProcessor:
    """è·å–å…¨å±€æ‰¹æ¬¡å¤„ç†å™¨å®ä¾‹"""
    global _global_batch_processor
    
    if _global_batch_processor is None or config is not None:
        _global_batch_processor = OptimizedBatchProcessor(config)
    
    return _global_batch_processor

async def initialize_batch_processor() -> OptimizedBatchProcessor:
    """åˆå§‹åŒ–å…¨å±€æ‰¹æ¬¡å¤„ç†å™¨"""
    return await get_batch_processor()
