"""
è¯­éŸ³è¯†åˆ«æ¨¡å‹åŠ è½½å’Œæ¨ç†æ¨¡å—

åŸºäºSherpa-ONNXå®ç°ASRæ¨¡å‹çš„å°è£…ï¼Œæ”¯æŒï¼š
- ç¦»çº¿/éæµå¼è¯­éŸ³è¯†åˆ« (SenseVoice, Paraformer, Whisperç­‰)
- åœ¨çº¿/æµå¼è¯­éŸ³è¯†åˆ« (Zipformerç­‰)
- å£°çº¹è¯†åˆ«å’Œè¯´è¯äººè¯†åˆ«
- è¯­éŸ³æ´»åŠ¨æ£€æµ‹ (VAD)
- å¤šç§æ¨ç†åç«¯ (CPU, CUDA, ONNX Runtime)
"""

import logging
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading

import numpy as np
import sherpa_onnx

from app.config import settings
from app.core.batch_processor import OptimizedBatchProcessor, get_batch_processor

logger = logging.getLogger(__name__)


class ModelLoadError(Exception):
    """æ¨¡å‹åŠ è½½å¼‚å¸¸"""
    pass


class InferenceError(Exception):
    """æ¨ç†å¼‚å¸¸"""
    pass


class ASRModelManager:
    """ASRæ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.offline_recognizer: Optional[sherpa_onnx.OfflineRecognizer] = None
        self.online_recognizer: Optional[sherpa_onnx.OnlineRecognizer] = None
        self.speaker_extractor: Optional[sherpa_onnx.SpeakerEmbeddingExtractor] = None
        self.speaker_manager: Optional[sherpa_onnx.SpeakerEmbeddingManager] = None
        self.punctuation_processor: Optional[sherpa_onnx.OfflinePunctuation] = None
        self.batch_processor: Optional[OptimizedBatchProcessor] = None
        
        self._is_initialized = False
        self._lock = threading.Lock()
        self._thread_pool = ThreadPoolExecutor(
            max_workers=settings.MAX_WORKERS,
            thread_name_prefix="asr_inference"
        )
        
        logger.info("ASRæ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    async def initialize(
        self,
        model_type: str = "sense_voice",
        use_gpu: bool = False,
        enable_vad: bool = True,
        enable_speaker_id: bool = False,
        enable_punctuation: bool = False
    ) -> None:
        """
        å¼‚æ­¥åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ (sense_voice, paraformer, whisper, zipformerç­‰)
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
            enable_vad: æ˜¯å¦å¯ç”¨VAD
            enable_speaker_id: æ˜¯å¦å¯ç”¨å£°çº¹è¯†åˆ«
            enable_punctuation: æ˜¯å¦å¯ç”¨æ ‡ç‚¹ç¬¦å·å¤„ç†
        """
        if self._is_initialized:
            logger.warning("æ¨¡å‹å·²ç»åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return
            
        logger.info(f"å¼€å§‹åˆå§‹åŒ–ASRæ¨¡å‹: {model_type}")
        start_time = time.time()
        
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ¨¡å‹åŠ è½½ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                self._thread_pool,
                self._load_models,
                model_type,
                use_gpu,
                enable_vad,
                enable_speaker_id,
                enable_punctuation
            )
            
            # åˆå§‹åŒ–æ‰¹æ¬¡å¤„ç†å™¨
            self.batch_processor = await get_batch_processor()
            
            # ä¿å­˜åˆå§‹åŒ–å‚æ•°
            self._model_type = model_type
            self._use_gpu = use_gpu
            self._is_initialized = True
            elapsed = time.time() - start_time
            logger.info(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise ModelLoadError(f"Failed to initialize models: {e}")

    def _load_models(
        self,
        model_type: str,
        use_gpu: bool,
        enable_vad: bool,
        enable_speaker_id: bool,
        enable_punctuation: bool
    ) -> None:
        """åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œçš„åŒæ­¥æ¨¡å‹åŠ è½½"""
        
        provider = "cuda" if use_gpu else "cpu"
        logger.info(f"ä½¿ç”¨æ¨ç†è®¾å¤‡: {provider}")
        
        # 1. åŠ è½½ç¦»çº¿ASRæ¨¡å‹
        self._load_offline_asr(model_type, provider)
        
        # 2. VADæ¨¡å‹ç”±ç»Ÿä¸€çš„VADProcessorç®¡ç†ï¼Œæ— éœ€åœ¨è¿™é‡ŒåŠ è½½
            
        # 3. åŠ è½½å£°çº¹è¯†åˆ«æ¨¡å‹
        if enable_speaker_id:
            self._load_speaker_models(provider)
            
        # 4. åŠ è½½æ ‡ç‚¹ç¬¦å·å¤„ç†æ¨¡å‹
        if enable_punctuation:
            self._load_punctuation_model(provider)

    def _load_offline_asr(self, model_type: str, provider: str) -> None:
        """åŠ è½½ç¦»çº¿ASRè¯†åˆ«å™¨"""
        try:
            if model_type == "sense_voice":
                # SenseVoiceæ¨¡å‹ - æ”¯æŒä¸­è‹±æ—¥éŸ©ç²¤å¤šè¯­è¨€
                model_path = Path(settings.ASR_MODEL_PATH) / "model.int8.onnx"
                tokens_path = Path(settings.ASR_MODEL_PATH) / "tokens.txt"
                
                if not model_path.exists() or not tokens_path.exists():
                    raise FileNotFoundError(f"SenseVoiceæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                    
                self.offline_recognizer = sherpa_onnx.OfflineRecognizer.from_sense_voice(
                    model=str(model_path),
                    tokens=str(tokens_path),
                    num_threads=settings.ASR_THREADS_PER_BATCH,
                    provider=provider,
                    language="auto",  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
                    use_itn=True,     # ä½¿ç”¨é€†æ–‡æœ¬è§„èŒƒåŒ–
                    debug=False
                )
                
            elif model_type == "paraformer":
                # Paraformeræ¨¡å‹ - é˜¿é‡Œè¾¾æ‘©é™¢æ¨¡å‹
                model_path = Path(settings.ASR_MODEL_PATH) / "model.onnx"
                tokens_path = Path(settings.ASR_MODEL_PATH) / "tokens.txt"
                
                if not model_path.exists() or not tokens_path.exists():
                    raise FileNotFoundError(f"Paraformeræ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                    
                self.offline_recognizer = sherpa_onnx.OfflineRecognizer.from_paraformer(
                    paraformer=str(model_path),
                    tokens=str(tokens_path),
                    num_threads=settings.ASR_THREADS_PER_BATCH,
                    sample_rate=settings.SAMPLE_RATE,
                    feature_dim=80,
                    decoding_method="greedy_search",
                    provider=provider,
                    debug=False
                )
                
            elif model_type == "whisper":
                # Whisperæ¨¡å‹
                encoder_path = Path(settings.ASR_MODEL_PATH) / "encoder.onnx"
                decoder_path = Path(settings.ASR_MODEL_PATH) / "decoder.onnx"
                tokens_path = Path(settings.ASR_MODEL_PATH) / "tokens.txt"
                
                if not all(p.exists() for p in [encoder_path, decoder_path, tokens_path]):
                    raise FileNotFoundError(f"Whisperæ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´")
                    
                self.offline_recognizer = sherpa_onnx.OfflineRecognizer.from_whisper(
                    encoder=str(encoder_path),
                    decoder=str(decoder_path),
                    tokens=str(tokens_path),
                    num_threads=settings.ASR_THREADS_PER_BATCH,
                    decoding_method="greedy_search",
                    language="",  # è‡ªåŠ¨æ£€æµ‹
                    task="transcribe",
                    provider=provider,
                    debug=False
                )
                
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                
            logger.info(f"ç¦»çº¿ASRæ¨¡å‹åŠ è½½æˆåŠŸ: {model_type}")
            
        except Exception as e:
            logger.error(f"åŠ è½½ç¦»çº¿ASRæ¨¡å‹å¤±è´¥: {e}")
            raise ModelLoadError(f"Failed to load offline ASR model: {e}")


    def _load_speaker_models(self, provider: str) -> None:
        """åŠ è½½å£°çº¹è¯†åˆ«æ¨¡å‹"""
        try:
            speaker_model_path = Path(settings.SPEAKER_MODEL_PATH)
            
            if not speaker_model_path.exists():
                raise FileNotFoundError(f"å£°çº¹æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {speaker_model_path}")
                
            # åˆ›å»ºå£°çº¹åµŒå…¥æå–å™¨
            config = sherpa_onnx.SpeakerEmbeddingExtractorConfig()
            config.model = str(speaker_model_path)
            config.provider = provider
            config.num_threads = 2
            
            self.speaker_extractor = sherpa_onnx.SpeakerEmbeddingExtractor(config)
            
            # åˆ›å»ºå£°çº¹ç®¡ç†å™¨
            self.speaker_manager = sherpa_onnx.SpeakerEmbeddingManager(
                self.speaker_extractor.dim
            )
            
            logger.info(f"å£°çº¹è¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸ (åµŒå…¥ç»´åº¦: {self.speaker_extractor.dim})")
            
        except Exception as e:
            logger.error(f"åŠ è½½å£°çº¹è¯†åˆ«æ¨¡å‹å¤±è´¥: {e}")
            raise ModelLoadError(f"Failed to load speaker model: {e}")

    def _load_punctuation_model(self, provider: str) -> None:
        """åŠ è½½æ ‡ç‚¹ç¬¦å·å¤„ç†æ¨¡å‹"""
        try:
            punctuation_model_path = Path(settings.PUNCTUATION_MODEL_PATH)

            if not punctuation_model_path.exists():
                raise FileNotFoundError(f"æ ‡ç‚¹ç¬¦å·æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {punctuation_model_path}")

            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
            model_file = punctuation_model_path / "model.int8.onnx"
            tokens_file = punctuation_model_path / "tokens.json"

            if not model_file.exists():
                raise FileNotFoundError(f"æ ‡ç‚¹ç¬¦å·æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file}")
            if not tokens_file.exists():
                raise FileNotFoundError(f"æ ‡ç‚¹ç¬¦å·tokensæ–‡ä»¶ä¸å­˜åœ¨: {tokens_file}")

            # åˆ›å»ºæ ‡ç‚¹ç¬¦å·å¤„ç†å™¨é…ç½®
            config = sherpa_onnx.OfflinePunctuationConfig()
            config.model.ct_transformer = str(model_file)
            config.model.num_threads = settings.PUNCTUATION_THREADS_PER_BATCH
            config.model.provider = provider
            config.model.debug = False

            # åˆ›å»ºæ ‡ç‚¹ç¬¦å·å¤„ç†å™¨
            self.punctuation_processor = sherpa_onnx.OfflinePunctuation(config)

            logger.info(f"æ ‡ç‚¹ç¬¦å·å¤„ç†æ¨¡å‹åŠ è½½æˆåŠŸ: {model_file}")

        except Exception as e:
            logger.error(f"åŠ è½½æ ‡ç‚¹ç¬¦å·æ¨¡å‹å¤±è´¥: {e}")
            raise ModelLoadError(f"Failed to load punctuation model: {e}")

    async def recognize_audio(
        self,
        audio_data: Union[np.ndarray, bytes],
        sample_rate: int = None,
        enable_vad: bool = True,
        enable_speaker_id: bool = False,
        enable_punctuation: bool = True
    ) -> Dict[str, Any]:
        """
        å¼‚æ­¥éŸ³é¢‘è¯†åˆ«æ¥å£
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ® (numpyæ•°ç»„æˆ–å­—èŠ‚æµ)
            sample_rate: é‡‡æ ·ç‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            enable_vad: æ˜¯å¦å¯ç”¨VADè¯­éŸ³æ®µè½åˆ†å‰²
            enable_speaker_id: æ˜¯å¦å¯ç”¨å£°çº¹è¯†åˆ«
            enable_punctuation: æ˜¯å¦å¯ç”¨æ ‡ç‚¹ç¬¦å·å¤„ç†

        Returns:
            è¯†åˆ«ç»“æœå­—å…¸ï¼ŒåŒ…å«æ–‡æœ¬ã€æ—¶é—´æˆ³ã€å£°çº¹ç­‰ä¿¡æ¯
        """
        if not self._is_initialized:
            raise InferenceError("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨initialize()")
            
        if self.offline_recognizer is None:
            raise InferenceError("ç¦»çº¿è¯†åˆ«å™¨æœªåŠ è½½")
            
        # é¢„å¤„ç†éŸ³é¢‘æ•°æ®
        if isinstance(audio_data, bytes):
            audio_samples = np.frombuffer(audio_data, dtype=np.float32)
        else:
            audio_samples = audio_data.astype(np.float32)
            
        if sample_rate is None:
            sample_rate = settings.SAMPLE_RATE
            
        # æ‰§è¡Œå¼‚æ­¥æ¨ç†
        result = await self._perform_recognition(
            audio_samples,
            sample_rate,
            enable_vad,
            enable_speaker_id,
            enable_punctuation
        )
        
        return result

    async def _perform_recognition(
        self,
        audio_samples: np.ndarray,
        sample_rate: int,
        enable_vad: bool,
        enable_speaker_id: bool,
        enable_punctuation: bool = True
    ) -> Dict[str, Any]:
        """æ‰§è¡Œå¼‚æ­¥è¯†åˆ«æ¨ç†"""
        
        start_time = time.time()
        
        try:
            if enable_vad:
                # ä½¿ç”¨VADåˆ†å‰²éŸ³é¢‘
                segments = await self._segment_audio_with_vad(audio_samples, sample_rate)
            else:
                # ä¸ä½¿ç”¨VADï¼Œæ•´æ®µè¯†åˆ«
                segments = [{
                    'samples': audio_samples,
                    'sample_rate': sample_rate,
                    'start_time': 0.0,
                    'end_time': len(audio_samples) / sample_rate
                }]
            
            # ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤„ç†å™¨è¿›è¡ŒäºŒé˜¶æ®µå¹¶è¡Œå¤„ç†
            if self.batch_processor:
                results = await self.batch_processor.process_segments_optimized(
                    segments,
                    enable_punctuation=enable_punctuation,
                    enable_speaker_id=enable_speaker_id,
                    asr_model=self,
                    punctuation_processor=self.punctuation_processor,
                    speaker_extractor=self.speaker_extractor
                )
            else:
                # é™çº§åˆ°ä¼ ç»Ÿæ‰¹æ¬¡å¤„ç†
                logger.warning("æ‰¹æ¬¡å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨ä¼ ç»Ÿå¤„ç†æ–¹å¼")
                results = await self._parallel_recognize_segments(
                    segments,
                    enable_speaker_id,
                    enable_punctuation,  # åœ¨å¹¶è¡Œå¤„ç†ä¸­ç›´æ¥å¤„ç†æ ‡ç‚¹ç¬¦å·
                    max_workers=4  # ä½¿ç”¨4ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†
                )

            # è®¡ç®—å¤„ç†ç»Ÿè®¡ä¿¡æ¯
            total_duration = len(audio_samples) / sample_rate
            processing_time = time.time() - start_time
            rtf = processing_time / total_duration if total_duration > 0 else 0
            
            return {
                'success': True,
                'results': results,
                'statistics': {
                    'total_duration': total_duration,
                    'processing_time': processing_time,
                    'real_time_factor': rtf,
                    'segments_count': len(results)
                }
            }
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘è¯†åˆ«å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'results': [],
                'statistics': {}
            }

    async def _segment_audio_with_vad(
        self,
        audio_samples: np.ndarray,
        sample_rate: int
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ç»Ÿä¸€VADå¤„ç†å™¨åˆ†å‰²éŸ³é¢‘"""

        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„VADå¤„ç†å™¨
            from app.core.vad import get_vad_processor
            
            vad_processor = await get_vad_processor()
            speech_segments = await vad_processor.detect_speech_segments(
                audio_samples, 
                sample_rate, 
                return_samples=True  # ASRéœ€è¦éŸ³é¢‘æ ·æœ¬
            )
            
            # è½¬æ¢ä¸ºASRæœŸæœ›çš„æ ¼å¼
            segments = []
            for segment in speech_segments:
                segments.append({
                    'samples': segment.samples if segment.samples is not None else audio_samples[
                        int(segment.start * sample_rate):int(segment.end * sample_rate)
                    ],
                    'sample_rate': sample_rate,
                    'start_time': segment.start,
                    'end_time': segment.end
                })
            
            if not segments:
                # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³æ®µè½ï¼Œè¿”å›æ•´æ®µéŸ³é¢‘
                logger.info("VADæœªæ£€æµ‹åˆ°è¯­éŸ³æ®µè½ï¼Œä½¿ç”¨æ•´æ®µéŸ³é¢‘")
                segments = [{
                    'samples': audio_samples,
                    'sample_rate': sample_rate,
                    'start_time': 0.0,
                    'end_time': len(audio_samples) / sample_rate
                }]
            
            logger.debug(f"VADåˆ†å‰²å®Œæˆ: æ£€æµ‹åˆ° {len(segments)} ä¸ªè¯­éŸ³æ®µè½")
            return segments

        except Exception as e:
            logger.error(f"VADåˆ†å‰²å¤±è´¥: {e}ï¼Œä½¿ç”¨æ•´æ®µéŸ³é¢‘")
            # VADå¤±è´¥æ—¶è¿”å›æ•´æ®µéŸ³é¢‘
            return [{
                'samples': audio_samples,
                'sample_rate': sample_rate,
                'start_time': 0.0,
                'end_time': len(audio_samples) / sample_rate
            }]

    async def _parallel_recognize_segments(
        self,
        segments: List[Dict[str, Any]],
        enable_speaker_id: bool,
        enable_punctuation: bool = True,
        max_workers: int = 4
    ) -> List[Dict[str, Any]]:
        """å¹¶è¡Œå¤„ç†å¤šä¸ªéŸ³é¢‘æ®µè½çš„è¯†åˆ« - å‚è€ƒdemoå®ç°"""

        if not segments:
            return []

        total_segments = len(segments)
        logger.info(f"å¼€å§‹å¹¶è¡Œå¤„ç† {total_segments} ä¸ªéŸ³é¢‘æ®µè½")

        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°åŠ¨æ€è®¡ç®—æ‰¹æ¬¡å¤§å°å’Œçº¿ç¨‹æ•°
        if total_segments <= 10:
            # å°é‡æ•°æ®ï¼šç›´æ¥å¤„ç†ï¼Œä¸åˆ†æ‰¹
            batch_size = total_segments
            max_workers = 1
        else:
            # æ ¹æ®é…ç½®è®¡ç®—æ‰¹æ¬¡å¤§å°
            batch_size = max(
                settings.MIN_BATCH_SIZE,
                min(settings.MAX_BATCH_SIZE, total_segments // settings.MAX_BATCH_THREADS)
            )

            # æ ¹æ®é…ç½®é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°
            max_workers = min(
                settings.MAX_BATCH_THREADS,
                max(1, total_segments // batch_size + 1)
            )

        logger.info(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        logger.info(f"ğŸ“Š é…ç½®å‚æ•°: ASRçº¿ç¨‹={settings.ASR_THREADS_PER_BATCH}, æ ‡ç‚¹çº¿ç¨‹={settings.PUNCTUATION_THREADS_PER_BATCH}")

        # åˆ›å»ºæ‰¹æ¬¡ä»»åŠ¡
        batch_tasks = []
        for start_idx in range(0, total_segments, batch_size):
            end_idx = min(start_idx + batch_size, total_segments)
            current_batch = segments[start_idx:end_idx]
            batch_tasks.append((current_batch, start_idx))

        logger.info(f"ğŸ“¦ åˆ›å»ºäº† {len(batch_tasks)} ä¸ªæ‰¹æ¬¡")

        # ä½¿ç”¨asyncioåœ¨çº¿ç¨‹æ± ä¸­å¹¶è¡Œå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
        loop = asyncio.get_event_loop()
        all_results = []

        # åˆ›å»ºæ‰€æœ‰æ‰¹æ¬¡çš„å¹¶è¡Œä»»åŠ¡
        batch_futures = []
        for batch_idx, (batch, start_idx) in enumerate(batch_tasks):
            future = loop.run_in_executor(
                self._thread_pool,
                self._process_batch,
                batch,
                start_idx,
                batch_idx + 1,
                len(batch_tasks),
                enable_speaker_id,
                enable_punctuation
            )
            batch_futures.append(future)

        # ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ
        logger.info(f"â³ ç­‰å¾…æ‰€æœ‰ {len(batch_futures)} ä¸ªæ‰¹æ¬¡å®Œæˆ...")
        batch_results = await asyncio.gather(*batch_futures, return_exceptions=True)

        # æ”¶é›†ç»“æœ
        for i, result in enumerate(batch_results):
            if isinstance(result, Exception):
                logger.error(f"æ‰¹æ¬¡ {i+1} å¤„ç†å¤±è´¥: {result}")
            else:
                all_results.extend(result)

        # æŒ‰æ—¶é—´æˆ³æ’åºç»“æœ
        all_results.sort(key=lambda x: x['start_time'])

        logger.info(f"ğŸ‰ å¹¶è¡Œå¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(all_results)} ä¸ªæ®µè½")
        return all_results

    def _process_batch(
        self,
        batch_segments: List[Dict[str, Any]],
        start_idx: int,
        batch_idx: int,
        total_batches: int,
        enable_speaker_id: bool,
        enable_punctuation: bool
    ) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„è¯­éŸ³æ®µè½ - å‚è€ƒdemoå®ç°"""

        batch_start_time = time.time()
        logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_idx}/{total_batches}ï¼ŒåŒ…å« {len(batch_segments)} ä¸ªæ®µè½")

        try:
            # åˆ›å»ºè¯†åˆ«æµ - é¢„åˆ†é…å†…å­˜
            streams = []
            for segment in batch_segments:
                stream = self.offline_recognizer.create_stream()
                stream.accept_waveform(segment['sample_rate'], segment['samples'])
                streams.append(stream)

            # æ‰¹é‡è¯†åˆ« - è¿™æ˜¯çœŸæ­£çš„å¹¶è¡Œå¤„ç†
            self.offline_recognizer.decode_streams(streams)

            # æ‰¹é‡è·å–ç»“æœ
            batch_results = []
            for i, stream in enumerate(streams):
                result = stream.result
                segment = batch_segments[i]

                # å‡†å¤‡åŸºç¡€ç»“æœ
                result_data = {
                    'text': result.text,
                    'start_time': segment['start_time'],
                    'end_time': segment['end_time'],
                    'language': getattr(result, 'lang', 'unknown'),
                    'emotion': getattr(result, 'emotion', 'unknown'),
                    'event': getattr(result, 'event', 'unknown'),
                    'speaker': 'unknown'
                }

                # æ·»åŠ å£°çº¹è¯†åˆ«
                if enable_speaker_id and self.speaker_extractor is not None:
                    speaker_info = self._identify_speaker(segment['samples'], segment['sample_rate'])
                    result_data['speaker'] = speaker_info

                # åœ¨å•ä¸ªæ®µè½çº§åˆ«æ·»åŠ æ ‡ç‚¹ç¬¦å·å¤„ç†
                if enable_punctuation and self.punctuation_processor is not None and result_data['text'].strip():
                    try:
                        punctuated_text = self.punctuation_processor.add_punctuation(result_data['text'])
                        result_data['text_with_punct'] = punctuated_text
                        result_data['text'] = punctuated_text  # æ›´æ–°ä¸»è¦æ–‡æœ¬å­—æ®µ
                    except Exception as e:
                        logger.warning(f"æ®µè½æ ‡ç‚¹å¤„ç†å¤±è´¥: {e}")
                        result_data['text_with_punct'] = result_data['text']
                else:
                    result_data['text_with_punct'] = result_data['text']

                batch_results.append(result_data)

            batch_time = time.time() - batch_start_time
            logger.info(f"âœ… æ‰¹æ¬¡ {batch_idx} å®Œæˆï¼Œè€—æ—¶ {batch_time:.2f}ç§’")

            return batch_results

        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯å ä½ç»“æœ
            error_results = []
            for segment in batch_segments:
                error_result = {
                    'text': '',
                    'start_time': segment['start_time'],
                    'end_time': segment['end_time'],
                    'language': 'unknown',
                    'emotion': 'unknown',
                    'event': 'unknown',
                    'speaker': 'unknown',
                    'text_with_punct': '',
                    'error': str(e)
                }
                error_results.append(error_result)
            return error_results




    def _identify_speaker(self, audio_samples: np.ndarray, sample_rate: int) -> str:
        """è¯†åˆ«è¯´è¯äºº"""
        
        if self.speaker_extractor is None or self.speaker_manager is None:
            return 'unknown'
        
        try:
            # æå–è¯´è¯äººåµŒå…¥
            stream = self.speaker_extractor.create_stream()
            stream.accept_waveform(sample_rate, audio_samples)
            stream.input_finished()
            
            embedding = self.speaker_extractor.compute(stream)
            if isinstance(embedding, list):
                embedding = np.array(embedding)
            
            # åœ¨å·²æ³¨å†Œè¯´è¯äººä¸­æœç´¢
            embedding_list = embedding.tolist()
            matched_speaker = self.speaker_manager.search(embedding_list, threshold=0.3)
            
            if matched_speaker:
                return matched_speaker
            else:
                # æœªåŒ¹é…åˆ°å·²æ³¨å†Œè¯´è¯äººï¼Œè¿”å›ä¸´æ—¶æ ‡è¯†
                return f"Speaker_{hash(tuple(embedding_list[:10])) % 1000:03d}"
                
        except Exception as e:
            logger.error(f"å£°çº¹è¯†åˆ«å¤±è´¥: {e}")
            return 'unknown'

    async def register_speaker(
        self,
        speaker_name: str,
        audio_data: Union[np.ndarray, bytes],
        sample_rate: int = None
    ) -> bool:
        """
        å¼‚æ­¥æ³¨å†Œè¯´è¯äºº
        
        Args:
            speaker_name: è¯´è¯äººåç§°
            audio_data: éŸ³é¢‘æ•°æ®
            sample_rate: é‡‡æ ·ç‡
            
        Returns:
            æ³¨å†Œæ˜¯å¦æˆåŠŸ
        """
        if self.speaker_extractor is None or self.speaker_manager is None:
            logger.error("å£°çº¹è¯†åˆ«æ¨¡å‹æœªåŠ è½½")
            return False
        
        # é¢„å¤„ç†éŸ³é¢‘æ•°æ®
        if isinstance(audio_data, bytes):
            audio_samples = np.frombuffer(audio_data, dtype=np.float32)
        else:
            audio_samples = audio_data.astype(np.float32)
            
        if sample_rate is None:
            sample_rate = settings.SAMPLE_RATE
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ³¨å†Œ
        loop = asyncio.get_event_loop()
        success = await loop.run_in_executor(
            self._thread_pool,
            self._register_speaker_sync,
            speaker_name,
            audio_samples,
            sample_rate
        )
        
        return success

    def _register_speaker_sync(
        self,
        speaker_name: str,
        audio_samples: np.ndarray,
        sample_rate: int
    ) -> bool:
        """åŒæ­¥æ³¨å†Œè¯´è¯äºº"""
        try:
            # æå–åµŒå…¥
            stream = self.speaker_extractor.create_stream()
            stream.accept_waveform(sample_rate, audio_samples)
            stream.input_finished()
            
            embedding = self.speaker_extractor.compute(stream)
            if isinstance(embedding, list):
                embedding = np.array(embedding)
            
            # æ³¨å†Œåˆ°ç®¡ç†å™¨
            success = self.speaker_manager.add(speaker_name, embedding.tolist())
            
            if success:
                logger.info(f"è¯´è¯äºº '{speaker_name}' æ³¨å†ŒæˆåŠŸ")
            else:
                logger.warning(f"è¯´è¯äºº '{speaker_name}' æ³¨å†Œå¤±è´¥")
                
            return success
            
        except Exception as e:
            logger.error(f"æ³¨å†Œè¯´è¯äºº '{speaker_name}' æ—¶å‡ºé”™: {e}")
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'is_initialized': self._is_initialized,
            'model_type': getattr(self, '_model_type', 'none'),
            'use_gpu': getattr(self, '_use_gpu', False),
            'asr_loaded': self.offline_recognizer is not None,
            'vad_loaded': True,  # VADç”±ç»Ÿä¸€çš„VADProcessorç®¡ç†
            'speaker_loaded': self.speaker_extractor is not None and self.speaker_manager is not None,
            'punctuation_loaded': self.punctuation_processor is not None,
            'sample_rate': settings.SAMPLE_RATE,
            'max_workers': settings.MAX_WORKERS,
            'models_dir': settings.MODELS_DIR,
            'asr_model_path': settings.ASR_MODEL_PATH,
            'vad_model_path': settings.VAD_MODEL_PATH,
            'speaker_model_path': settings.SPEAKER_MODEL_PATH,
            'punctuation_model_path': settings.PUNCTUATION_MODEL_PATH
        }

    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        logger.info("æ­£åœ¨æ¸…ç†ASRæ¨¡å‹èµ„æº...")
        
        # å…³é—­çº¿ç¨‹æ± 
        self._thread_pool.shutdown(wait=True)
        
        # æ¸…ç†æ¨¡å‹å¼•ç”¨
        self.offline_recognizer = None
        self.online_recognizer = None
        self.speaker_extractor = None
        self.speaker_manager = None
        self.punctuation_processor = None
        
        self._is_initialized = False
        logger.info("ASRæ¨¡å‹èµ„æºæ¸…ç†å®Œæˆ")


# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = ASRModelManager()


async def initialize_models(**kwargs) -> None:
    """åˆå§‹åŒ–å…¨å±€æ¨¡å‹ç®¡ç†å™¨"""
    await model_manager.initialize(**kwargs)


async def recognize_audio(**kwargs) -> Dict[str, Any]:
    """å…¨å±€éŸ³é¢‘è¯†åˆ«æ¥å£"""
    return await model_manager.recognize_audio(**kwargs)


async def register_speaker(**kwargs) -> bool:
    """å…¨å±€è¯´è¯äººæ³¨å†Œæ¥å£"""
    return await model_manager.register_speaker(**kwargs)


def get_model_info() -> Dict[str, Any]:
    """è·å–å…¨å±€æ¨¡å‹ä¿¡æ¯"""
    return model_manager.get_model_info()


async def cleanup_models() -> None:
    """æ¸…ç†å…¨å±€æ¨¡å‹èµ„æº"""
    await model_manager.cleanup()
